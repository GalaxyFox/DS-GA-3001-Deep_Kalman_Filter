{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16fbd39d550>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "vae_batch_size = 512\n",
    "seed = 1\n",
    "epochs = 200\n",
    "rnn_epochs = 10\n",
    "vae_epochs = 50\n",
    "cuda = True\n",
    "log_interval = 10\n",
    "sample_size = 10\n",
    "h_d = 512\n",
    "l_d = 32\n",
    "u_d = 1\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if cuda else {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hmnist dataset\n",
    "import healing_mnist_indep\n",
    "import vae_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hmnist = healing_mnist_indep.HealingMNIST(seq_len=5, # 5 rotations of each digit\n",
    "                                          square_count=0, # 3 out of 5 images have a square added to them\n",
    "                                          square_size=5, # the square is 5x5\n",
    "                                          noise_ratio=0.1, # on average, 20% of the image is eaten by noise,\n",
    "                                          digits=range(10), # only include this digits\n",
    "                                          test = False\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae = vae_mnist.HealingMNIST(seq_len=5, # 5 rotations of each digit\n",
    "                                          square_count=0, # 3 out of 5 images have a square added to them\n",
    "                                          square_size=5, # the square is 5x5\n",
    "                                          noise_ratio=0.10, # on average, 20% of the image is eaten by noise,\n",
    "                                          digits=range(10), # only include this digits\n",
    "                                          test = False\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5, 28, 28)\n",
      "(60000, 5)\n",
      "(10000, 5, 28, 28) (10000, 5, 28, 28)\n",
      "(10000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(hmnist.train_images.shape,hmnist.train_targets.shape)\n",
    "print(hmnist.train_rotations.shape)\n",
    "print(hmnist.test_images.shape,hmnist.test_targets.shape)\n",
    "print(hmnist.test_rotations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 6, 28, 28) (60000, 6, 28, 28)\n",
      "(10000, 6, 28, 28) (10000, 6, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(vae.train_images.shape,vae.train_targets.shape)\n",
    "\n",
    "print(vae.test_images.shape,vae.test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtsAAACaCAYAAAB464RIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgAAAJz8H2Ewn0evjSSGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x576 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "case = 4\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "for i, image in enumerate(hmnist.test_images[case]):\n",
    "    fig.add_subplot(1, 6, i+1)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtsAAACaCAYAAAB464RIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2QYAgE402wAA0Mn/AP0wFFnRJHTzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x576 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "for i, image in enumerate(hmnist.test_targets[case]):\n",
    "    fig.add_subplot(1, 6, i+1)\n",
    "    plt.imshow(image,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 65.88950329  82.17884058 -44.44024224  23.42226093  27.14671693]\n"
     ]
    }
   ],
   "source": [
    "print(hmnist.test_rotations[case])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAACICAYAAACBSN//AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAA/lf3wLH8Uct9NEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x576 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "case = 17\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "for i, image in enumerate(vae.test_images[case]):\n",
    "    fig.add_subplot(1, 7, i+1)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAACICAYAAACBSN//AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMBATeAAAGMj/AfPg3GcdvIDTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x576 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "for i, image in enumerate(vae.test_targets[case]):\n",
    "    fig.add_subplot(1, 7, i+1)\n",
    "    plt.imshow(image,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the data shape\n",
    "this part should be different for different model, the q-RNN model does not igorned the sequencial dependency within the dataset, so we don't need to flat the dataset. So that the dataset should be that given a sequence of noisy image $\\{p_i\\}$ and a sequences of action $\\{u_i\\}$, the target should be the image of next timestep given action $u_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5) (60000, 5, 28, 28)\n",
      "(10000, 5, 28, 28) (10000, 5) (10000, 5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "train_X = hmnist.train_images\n",
    "train_u = hmnist.train_rotations\n",
    "train_Y = hmnist.train_targets\n",
    "test_X = hmnist.test_images\n",
    "test_u = hmnist.test_rotations\n",
    "test_Y = hmnist.test_targets\n",
    "print(train_X.shape,train_u.shape,train_Y.shape)\n",
    "print(test_X.shape,test_u.shape,test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5) (60000, 5, 28, 28)\n",
      "(10000, 5, 28, 28) (10000, 5) (10000, 5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#no adjustment needed\n",
    "\n",
    "#train_Y = train_Y[:,4,:,:]\n",
    "#test_Y = test_Y[:,4,:,:]\n",
    "\n",
    "print(train_X.shape,train_u.shape,train_Y.shape)\n",
    "print(test_X.shape,test_u.shape,test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 6, 28, 28) (60000, 6, 28, 28)\n",
      "(10000, 6, 28, 28) (10000, 6, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "vae_train_X = vae.train_images\n",
    "vae_train_Y = vae.train_targets\n",
    "vae_test_X = vae.test_images\n",
    "vae_test_Y = vae.test_targets\n",
    "print(vae_train_X.shape,vae_train_Y.shape)\n",
    "print(vae_test_X.shape,vae_test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360000, 28, 28) (360000, 28, 28)\n",
      "(60000, 28, 28) (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#for the above six parameter, flaten the first two dimension\n",
    "vae_train_X = vae_train_X.reshape(-1,28,28)\n",
    "vae_train_Y = vae_train_Y.reshape(-1,28,28)\n",
    "vae_test_X = vae_test_X.reshape(-1,28,28)\n",
    "vae_test_Y = vae_test_Y.reshape(-1,28,28)\n",
    "print(vae_train_X.shape,vae_train_Y.shape)\n",
    "print(vae_test_X.shape,vae_test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HMNISTDataSet():\n",
    "    def __init__(self, train_img, train_act, train_tar, test_img, test_act, test_tar, test = False, transform=None):\n",
    "        self.test = test\n",
    "        self.transform = transform\n",
    "\n",
    "        if (self.test == False):\n",
    "          self.images = train_img\n",
    "          self.targets = train_tar\n",
    "          self.rotations = train_act\n",
    "\n",
    "        else:      \n",
    "          self.images = test_img\n",
    "          self.targets = test_tar\n",
    "          self.rotations = test_act\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform is not None:\n",
    "            img = torch.zeros((len(self.images[index]),1,28,28))\n",
    "            for i in range(len(self.images[index])):\n",
    "                img[i] = self.transform(self.images[index][i].reshape(28,28,1))\n",
    "            tar = torch.zeros((len(self.targets[index]),1,28,28))\n",
    "            for i in range(len(self.targets[index])):\n",
    "                tar[i] = self.transform(self.targets[index][i].reshape(28,28,1))\n",
    "                \n",
    "            rot = torch.tensor(self.rotations[index])\n",
    "        return img, rot, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAEDataSet():\n",
    "    def __init__(self, train_img, train_tar, test_img, test_tar, test = False, transform=None):\n",
    "        self.test = test\n",
    "        self.transform = transform\n",
    "\n",
    "        if (self.test == False):\n",
    "          self.images = train_img\n",
    "          self.targets = train_tar\n",
    "\n",
    "\n",
    "        else:      \n",
    "          self.images = test_img\n",
    "          self.targets = test_tar\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(self.images[index].reshape(28,28,1))\n",
    "            tar = self.transform(self.targets[index].reshape(28,28,1))\n",
    "        return img, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = HMNISTDataSet(train_X, train_u, train_Y, test_X, test_u, test_Y, test = False, transform = transforms.ToTensor())\n",
    "test_set = HMNISTDataSet(train_X, train_u, train_Y, test_X, test_u, test_Y, test = True, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae_train_set = VAEDataSet(vae_train_X, vae_train_Y, vae_test_X, vae_test_Y, test = False, transform = transforms.ToTensor())\n",
    "vae_test_set = VAEDataSet(vae_train_X, vae_train_Y, vae_test_X, vae_test_Y, test = True, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b = vae_train_set.__getitem__(3)\n",
    "b.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae_train_loader = torch.utils.data.DataLoader(vae_train_set, batch_size=vae_batch_size, shuffle=True, **kwargs)\n",
    "vae_test_loader = torch.utils.data.DataLoader(vae_test_set, batch_size=vae_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the purpose of this model is to examine whether pretrain vae is possible or not\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, h_d)\n",
    "        self.fc2 = nn.Linear(h_d,128)\n",
    "        self.fc21 = nn.Linear(128, l_d)\n",
    "        self.fc22 = nn.Linear(128, l_d)\n",
    "        \n",
    "        #transition layer\n",
    "        input_dim = l_d + u_d\n",
    "        self.rnn_mu = nn.RNN(input_size=input_dim,hidden_size=l_d,batch_first=True)\n",
    "        self.rnn_sigma = nn.RNN(input_size=input_dim,hidden_size=l_d,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(l_d, 128)\n",
    "        self.fc4 = nn.Linear(128,h_d)\n",
    "        self.fc5 = nn.Linear(h_d, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.float()\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc21(h2), self.fc22(h2)\n",
    "\n",
    "    def reparameterize1(self, mu, logvar, n=1):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def transition(self, z, u):\n",
    "        rnn_input = torch.cat((z,u),dim=2)\n",
    "        mu2,_ = self.rnn_mu(rnn_input)\n",
    "        logvar2,_ = self.rnn_sigma(rnn_input)\n",
    "        return mu2,logvar2\n",
    "    \n",
    "    def reparameterize2(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        h4 = F.relu(self.fc4(h3))\n",
    "        return torch.sigmoid(self.fc5(h4))\n",
    "\n",
    "    def forward(self, x=None,mu_i=None,logvar_i=None,u=None, vae=False, rnn=False):\n",
    "        if vae:\n",
    "            mu1, logvar1 = self.encode(x.view(-1, 784))\n",
    "            z1 = self.reparameterize1(mu1, logvar1)\n",
    "            return self.decode(z1), mu1, logvar1\n",
    "        elif rnn:\n",
    "            mu = torch.empty(sample_size, x.shape[0], x.shape[1], l_d , dtype=torch.float).to(device)\n",
    "            logvar = torch.empty(sample_size, x.shape[0], x.shape[1], l_d , dtype=torch.float).to(device)\n",
    "            for i in range(sample_size):\n",
    "                z1 = self.reparameterize1(mu_i, logvar_i)\n",
    "                z1 = z1.reshape(-1,5,32)\n",
    "                u = u.float()\n",
    "                mu2, logvar2 = self.transition(z1,u.reshape(-1,5,1))\n",
    "                mu[i] = mu2\n",
    "                logvar[i] = logvar2\n",
    "            return mu, logvar\n",
    "        else:\n",
    "            mu1, logvar1 = self.encode(x.view(-1, 784))\n",
    "            mu = torch.empty(sample_size, x.shape[0], x.shape[1], l_d , dtype=torch.float).to(device)\n",
    "            logvar = torch.empty(sample_size, x.shape[0], x.shape[1], l_d , dtype=torch.float).to(device)\n",
    "            for i in range(sample_size):\n",
    "                z1 = self.reparameterize1(mu1, logvar1)\n",
    "                z1 = z1.reshape(-1,5,32)\n",
    "                u = u.float()\n",
    "                mu2, logvar2 = self.transition(z1,u.reshape(-1,5,1))\n",
    "                mu[i] = mu2\n",
    "                logvar[i] = logvar2\n",
    "            z2 = self.reparameterize2(mu[0], logvar[0])\n",
    "            return self.decode(z2), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar[0,:,0,:] - mu[0,:,0,:].pow(2) - logvar[0,:,0,:].exp())\n",
    "    EKLD = 0\n",
    "    for i in range(1,5):        \n",
    "        tmp = -0.5 * torch.sum(1 + logvar[:,:,i,:] - mu[:,:,i,:].pow(2) - logvar[:,:,i,:].exp())/sample_size\n",
    "        EKLD += tmp\n",
    "    return BCE + KLD + EKLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    x = x.float()\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def rnn_loss_function(mu, logvar):\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar[0,:,0,:] - mu[0,:,0,:].pow(2) - logvar[0,:,0,:].exp())\n",
    "    EKLD = 0\n",
    "    for i in range(1,5):        \n",
    "        tmp = -0.5 * torch.sum(1 + logvar[:,:,i,:] - mu[:,:,i,:].pow(2) - logvar[:,:,i,:].exp())/sample_size\n",
    "        EKLD += tmp\n",
    "    return KLD + EKLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_vae():\n",
    "    #freeze the rnn layer first\n",
    "    count = 0\n",
    "    for child in model.children():\n",
    "        if count == 4 or count == 5:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False;\n",
    "        count += 1;\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(vae_epochs):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (img, target) in enumerate(vae_train_loader):\n",
    "            img = img.cuda()\n",
    "            target = target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model.forward(x=img,vae=True)\n",
    "            loss = vae_loss_function(recon_batch, target, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "              i + 1, train_loss / len(vae_train_loader.dataset)))\n",
    "    print(\"vae pretrain has been done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finish_vae():\n",
    "    for child in model.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True;\n",
    "    print(\"ready for rnn model train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_rnn():\n",
    "    #freeze the vae layer first\n",
    "    count = 0\n",
    "    for child in model.children():\n",
    "        if count != 4 or count != 5:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False;\n",
    "        count += 1;\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(rnn_epochs):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (img, action, target) in enumerate(train_loader):     \n",
    "            img = img.to(device)\n",
    "            action = action.to(device)\n",
    "            target = target.to(device)\n",
    "            with torch.no_grad():\n",
    "                mu, logvar = model.encode(img.view(-1, 784))\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            mu.requires_grad = True\n",
    "            logvar.requires_grad = True\n",
    "            mu1, logvar1 = model.forward(x=img,u=action,mu_i=mu,logvar_i=logvar,rnn=True)\n",
    "            loss = rnn_loss_function(mu1, logvar1)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "              i + 1, train_loss / len(vae_train_loader.dataset)))\n",
    "    print(\"rnn pretrain has been done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finish_rnn():\n",
    "    for child in model.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True;\n",
    "    print(\"ready for the whole model train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (img, action, target) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        action = action.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(x=img,u=action)\n",
    "        loss = loss_function(recon_batch, target, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        #if batch_idx % log_interval == 0:\n",
    "         #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          #      epoch, batch_idx * len(img), len(train_loader.dataset),\n",
    "           #     100. * batch_idx / len(train_loader),\n",
    "            #    loss.item() / len(img)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    return train_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (img, action, target) in enumerate(test_loader):\n",
    "            img = img.to(device)\n",
    "            action = action.to(device)\n",
    "            target = target.to(device)\n",
    "            recon_batch, mu, logvar = model(x=img, u=action)\n",
    "            test_loss += loss_function(recon_batch, target, mu, logvar).item()\n",
    "            if(epoch > epochs - 10):\n",
    "                if i == 0:\n",
    "                    n = np.random.randint(0,batch_size)\n",
    "                    comparison = torch.cat([target[n],\n",
    "                                          recon_batch.view(batch_size, 5, 1, 28, 28)[n]],dim=0)\n",
    "                    save_image(comparison.cpu(),\n",
    "                             'results/reconstruction_' + str(epoch) + '.png', nrow=5)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "#adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 192.0418\n",
      "====> Epoch: 2 Average loss: 142.9048\n",
      "====> Epoch: 3 Average loss: 129.1424\n",
      "====> Epoch: 4 Average loss: 122.7451\n",
      "====> Epoch: 5 Average loss: 118.9887\n",
      "====> Epoch: 6 Average loss: 116.3216\n",
      "====> Epoch: 7 Average loss: 114.4632\n",
      "====> Epoch: 8 Average loss: 113.1223\n",
      "====> Epoch: 9 Average loss: 112.0737\n",
      "====> Epoch: 10 Average loss: 111.2387\n",
      "====> Epoch: 11 Average loss: 110.5473\n",
      "====> Epoch: 12 Average loss: 109.9740\n",
      "====> Epoch: 13 Average loss: 109.4127\n",
      "====> Epoch: 14 Average loss: 109.0301\n",
      "====> Epoch: 15 Average loss: 108.6125\n",
      "====> Epoch: 16 Average loss: 108.2380\n",
      "====> Epoch: 17 Average loss: 107.8842\n",
      "====> Epoch: 18 Average loss: 107.6355\n",
      "====> Epoch: 19 Average loss: 107.3292\n",
      "====> Epoch: 20 Average loss: 107.0471\n",
      "====> Epoch: 21 Average loss: 106.8174\n",
      "====> Epoch: 22 Average loss: 106.6014\n",
      "====> Epoch: 23 Average loss: 106.4046\n",
      "====> Epoch: 24 Average loss: 106.1653\n",
      "====> Epoch: 25 Average loss: 105.9837\n",
      "====> Epoch: 26 Average loss: 105.8036\n",
      "====> Epoch: 27 Average loss: 105.5983\n",
      "====> Epoch: 28 Average loss: 105.4469\n",
      "====> Epoch: 29 Average loss: 105.3016\n",
      "====> Epoch: 30 Average loss: 105.1443\n",
      "====> Epoch: 31 Average loss: 105.0188\n",
      "====> Epoch: 32 Average loss: 104.8743\n",
      "====> Epoch: 33 Average loss: 104.7220\n",
      "====> Epoch: 34 Average loss: 104.5958\n",
      "====> Epoch: 35 Average loss: 104.4894\n",
      "====> Epoch: 36 Average loss: 104.3810\n",
      "====> Epoch: 37 Average loss: 104.2935\n",
      "====> Epoch: 38 Average loss: 104.1634\n",
      "====> Epoch: 39 Average loss: 104.0901\n",
      "====> Epoch: 40 Average loss: 103.9337\n",
      "====> Epoch: 41 Average loss: 103.8887\n",
      "====> Epoch: 42 Average loss: 103.7861\n",
      "====> Epoch: 43 Average loss: 103.7029\n",
      "====> Epoch: 44 Average loss: 103.6134\n",
      "====> Epoch: 45 Average loss: 103.5404\n",
      "====> Epoch: 46 Average loss: 103.4861\n",
      "====> Epoch: 47 Average loss: 103.3822\n",
      "====> Epoch: 48 Average loss: 103.3091\n",
      "====> Epoch: 49 Average loss: 103.2600\n",
      "====> Epoch: 50 Average loss: 103.1820\n",
      "vae pretrain has been done\n",
      "ready for rnn model train\n"
     ]
    }
   ],
   "source": [
    "train_vae()\n",
    "finish_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 16.6551\n",
      "====> Epoch: 2 Average loss: 16.6551\n",
      "====> Epoch: 3 Average loss: 16.6553\n",
      "====> Epoch: 4 Average loss: 16.6554\n",
      "====> Epoch: 5 Average loss: 16.6553\n",
      "====> Epoch: 6 Average loss: 16.6547\n",
      "====> Epoch: 7 Average loss: 16.6553\n",
      "====> Epoch: 8 Average loss: 16.6549\n",
      "====> Epoch: 9 Average loss: 16.6551\n",
      "====> Epoch: 10 Average loss: 16.6552\n",
      "rnn pretrain has been done\n",
      "ready for the whole model train\n"
     ]
    }
   ],
   "source": [
    "import warnings;\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_rnn()\n",
    "finish_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 1072.0767\n",
      "====> Test set loss: 977.3884\n",
      "====> Epoch: 2 Average loss: 957.7257\n",
      "====> Test set loss: 938.1437\n",
      "====> Epoch: 3 Average loss: 926.3934\n",
      "====> Test set loss: 911.8797\n",
      "====> Epoch: 4 Average loss: 904.6363\n",
      "====> Test set loss: 894.8882\n",
      "====> Epoch: 5 Average loss: 890.9514\n",
      "====> Test set loss: 884.1969\n",
      "====> Epoch: 6 Average loss: 881.7021\n",
      "====> Test set loss: 875.1841\n",
      "====> Epoch: 7 Average loss: 873.5355\n",
      "====> Test set loss: 868.2546\n",
      "====> Epoch: 8 Average loss: 866.3834\n",
      "====> Test set loss: 863.9958\n",
      "====> Epoch: 9 Average loss: 860.5858\n",
      "====> Test set loss: 857.3587\n",
      "====> Epoch: 10 Average loss: 856.0688\n",
      "====> Test set loss: 854.2608\n",
      "====> Epoch: 11 Average loss: 852.2928\n",
      "====> Test set loss: 851.9351\n",
      "====> Epoch: 12 Average loss: 848.4045\n",
      "====> Test set loss: 847.2105\n",
      "====> Epoch: 13 Average loss: 845.3711\n",
      "====> Test set loss: 843.7339\n",
      "====> Epoch: 14 Average loss: 842.5777\n",
      "====> Test set loss: 842.6448\n",
      "====> Epoch: 15 Average loss: 839.5112\n",
      "====> Test set loss: 839.0293\n",
      "====> Epoch: 16 Average loss: 836.9849\n",
      "====> Test set loss: 835.6211\n",
      "====> Epoch: 17 Average loss: 834.1467\n",
      "====> Test set loss: 833.7121\n",
      "====> Epoch: 18 Average loss: 831.9297\n",
      "====> Test set loss: 831.4548\n",
      "====> Epoch: 19 Average loss: 829.8986\n",
      "====> Test set loss: 831.1671\n",
      "====> Epoch: 20 Average loss: 828.0778\n",
      "====> Test set loss: 830.6036\n",
      "====> Epoch: 21 Average loss: 826.2480\n",
      "====> Test set loss: 828.1644\n",
      "====> Epoch: 22 Average loss: 824.8094\n",
      "====> Test set loss: 825.5743\n",
      "====> Epoch: 23 Average loss: 823.2270\n",
      "====> Test set loss: 824.2361\n",
      "====> Epoch: 24 Average loss: 821.7397\n",
      "====> Test set loss: 824.0125\n",
      "====> Epoch: 25 Average loss: 820.5843\n",
      "====> Test set loss: 821.7475\n",
      "====> Epoch: 26 Average loss: 819.1742\n",
      "====> Test set loss: 821.9183\n",
      "====> Epoch: 27 Average loss: 818.2837\n",
      "====> Test set loss: 819.7893\n",
      "====> Epoch: 28 Average loss: 816.9293\n",
      "====> Test set loss: 820.7075\n",
      "====> Epoch: 29 Average loss: 815.6391\n",
      "====> Test set loss: 818.5827\n",
      "====> Epoch: 30 Average loss: 814.3736\n",
      "====> Test set loss: 817.6707\n",
      "====> Epoch: 31 Average loss: 813.6482\n",
      "====> Test set loss: 816.2452\n",
      "====> Epoch: 32 Average loss: 812.4470\n",
      "====> Test set loss: 814.8732\n",
      "====> Epoch: 33 Average loss: 812.1449\n",
      "====> Test set loss: 816.3023\n",
      "====> Epoch: 34 Average loss: 811.2390\n",
      "====> Test set loss: 815.4422\n",
      "====> Epoch: 35 Average loss: 810.1015\n",
      "====> Test set loss: 814.1968\n",
      "====> Epoch: 36 Average loss: 809.4607\n",
      "====> Test set loss: 812.7025\n",
      "====> Epoch: 37 Average loss: 808.6264\n",
      "====> Test set loss: 813.4006\n",
      "====> Epoch: 38 Average loss: 807.9345\n",
      "====> Test set loss: 812.3508\n",
      "====> Epoch: 39 Average loss: 807.4795\n",
      "====> Test set loss: 811.6520\n",
      "====> Epoch: 40 Average loss: 806.7211\n",
      "====> Test set loss: 811.2866\n",
      "====> Epoch: 41 Average loss: 806.1455\n",
      "====> Test set loss: 810.2270\n",
      "====> Epoch: 42 Average loss: 805.5226\n",
      "====> Test set loss: 811.6577\n",
      "====> Epoch: 43 Average loss: 804.7187\n",
      "====> Test set loss: 809.2232\n",
      "====> Epoch: 44 Average loss: 804.2308\n",
      "====> Test set loss: 809.0304\n",
      "====> Epoch: 45 Average loss: 804.0004\n",
      "====> Test set loss: 808.5468\n",
      "====> Epoch: 46 Average loss: 803.2113\n",
      "====> Test set loss: 809.3900\n",
      "====> Epoch: 47 Average loss: 802.7732\n",
      "====> Test set loss: 810.7555\n",
      "====> Epoch: 48 Average loss: 802.7669\n",
      "====> Test set loss: 809.2414\n",
      "====> Epoch: 49 Average loss: 801.8096\n",
      "====> Test set loss: 808.2652\n",
      "====> Epoch: 50 Average loss: 801.7441\n",
      "====> Test set loss: 807.6940\n",
      "====> Epoch: 51 Average loss: 801.0499\n",
      "====> Test set loss: 805.9063\n",
      "====> Epoch: 52 Average loss: 800.5686\n",
      "====> Test set loss: 808.3279\n",
      "====> Epoch: 53 Average loss: 800.2696\n",
      "====> Test set loss: 807.4276\n",
      "====> Epoch: 54 Average loss: 799.4718\n",
      "====> Test set loss: 806.2159\n",
      "====> Epoch: 55 Average loss: 799.4730\n",
      "====> Test set loss: 806.3191\n",
      "====> Epoch: 56 Average loss: 799.0797\n",
      "====> Test set loss: 805.7455\n",
      "====> Epoch: 57 Average loss: 798.6346\n",
      "====> Test set loss: 806.1115\n",
      "====> Epoch: 58 Average loss: 798.3439\n",
      "====> Test set loss: 804.6137\n",
      "====> Epoch: 59 Average loss: 798.0584\n",
      "====> Test set loss: 807.3520\n",
      "====> Epoch: 60 Average loss: 797.4915\n",
      "====> Test set loss: 804.6816\n",
      "====> Epoch: 61 Average loss: 797.0561\n",
      "====> Test set loss: 805.6483\n",
      "====> Epoch: 62 Average loss: 796.9849\n",
      "====> Test set loss: 805.2328\n",
      "====> Epoch: 63 Average loss: 796.4167\n",
      "====> Test set loss: 806.1212\n",
      "====> Epoch: 64 Average loss: 796.2414\n",
      "====> Test set loss: 803.8748\n",
      "====> Epoch: 65 Average loss: 796.1870\n",
      "====> Test set loss: 805.2500\n",
      "====> Epoch: 66 Average loss: 795.6719\n",
      "====> Test set loss: 803.7503\n",
      "====> Epoch: 67 Average loss: 795.4027\n",
      "====> Test set loss: 803.1586\n",
      "====> Epoch: 68 Average loss: 794.8066\n",
      "====> Test set loss: 802.4711\n",
      "====> Epoch: 69 Average loss: 794.5387\n",
      "====> Test set loss: 802.8889\n",
      "====> Epoch: 70 Average loss: 794.2032\n",
      "====> Test set loss: 805.5210\n",
      "====> Epoch: 71 Average loss: 794.0547\n",
      "====> Test set loss: 803.1857\n",
      "====> Epoch: 72 Average loss: 793.6898\n",
      "====> Test set loss: 802.2795\n",
      "====> Epoch: 73 Average loss: 793.2781\n",
      "====> Test set loss: 801.6058\n",
      "====> Epoch: 74 Average loss: 793.0948\n",
      "====> Test set loss: 801.8845\n",
      "====> Epoch: 75 Average loss: 792.5238\n",
      "====> Test set loss: 801.4735\n",
      "====> Epoch: 76 Average loss: 792.8150\n",
      "====> Test set loss: 801.5955\n",
      "====> Epoch: 77 Average loss: 792.3693\n",
      "====> Test set loss: 803.7935\n",
      "====> Epoch: 78 Average loss: 791.8575\n",
      "====> Test set loss: 802.3850\n",
      "====> Epoch: 79 Average loss: 791.8921\n",
      "====> Test set loss: 802.1757\n",
      "====> Epoch: 80 Average loss: 791.8812\n",
      "====> Test set loss: 801.2205\n",
      "====> Epoch: 81 Average loss: 791.2851\n",
      "====> Test set loss: 801.4234\n",
      "====> Epoch: 82 Average loss: 791.5705\n",
      "====> Test set loss: 800.3028\n",
      "====> Epoch: 83 Average loss: 791.1798\n",
      "====> Test set loss: 801.2151\n",
      "====> Epoch: 84 Average loss: 791.0946\n",
      "====> Test set loss: 802.3814\n",
      "====> Epoch: 85 Average loss: 790.5799\n",
      "====> Test set loss: 801.5244\n",
      "====> Epoch: 86 Average loss: 790.7729\n",
      "====> Test set loss: 799.8554\n",
      "====> Epoch: 87 Average loss: 790.1482\n",
      "====> Test set loss: 800.2037\n",
      "====> Epoch: 88 Average loss: 790.1094\n",
      "====> Test set loss: 799.9854\n",
      "====> Epoch: 89 Average loss: 790.1317\n",
      "====> Test set loss: 801.3709\n",
      "====> Epoch: 90 Average loss: 789.9106\n",
      "====> Test set loss: 800.5617\n",
      "====> Epoch: 91 Average loss: 789.3949\n",
      "====> Test set loss: 800.3757\n",
      "====> Epoch: 92 Average loss: 789.8078\n",
      "====> Test set loss: 799.9814\n",
      "====> Epoch: 93 Average loss: 789.4528\n",
      "====> Test set loss: 798.9165\n",
      "====> Epoch: 94 Average loss: 788.8633\n",
      "====> Test set loss: 799.9446\n",
      "====> Epoch: 95 Average loss: 789.2431\n",
      "====> Test set loss: 799.0857\n",
      "====> Epoch: 96 Average loss: 788.7240\n",
      "====> Test set loss: 799.4690\n",
      "====> Epoch: 97 Average loss: 788.6142\n",
      "====> Test set loss: 801.4232\n",
      "====> Epoch: 98 Average loss: 788.2572\n",
      "====> Test set loss: 800.0654\n",
      "====> Epoch: 99 Average loss: 787.8646\n",
      "====> Test set loss: 799.3547\n",
      "====> Epoch: 100 Average loss: 787.8095\n",
      "====> Test set loss: 798.3896\n",
      "====> Epoch: 101 Average loss: 787.9710\n",
      "====> Test set loss: 799.9688\n",
      "====> Epoch: 102 Average loss: 787.5769\n",
      "====> Test set loss: 800.1506\n",
      "====> Epoch: 103 Average loss: 787.5997\n",
      "====> Test set loss: 799.1742\n",
      "====> Epoch: 104 Average loss: 787.9572\n",
      "====> Test set loss: 799.0206\n",
      "====> Epoch: 105 Average loss: 787.3692\n",
      "====> Test set loss: 799.4776\n",
      "====> Epoch: 106 Average loss: 786.6983\n",
      "====> Test set loss: 798.2656\n",
      "====> Epoch: 107 Average loss: 786.5105\n",
      "====> Test set loss: 799.2215\n",
      "====> Epoch: 108 Average loss: 787.1213\n",
      "====> Test set loss: 798.5279\n",
      "====> Epoch: 109 Average loss: 786.9454\n",
      "====> Test set loss: 798.6276\n",
      "====> Epoch: 110 Average loss: 786.6963\n",
      "====> Test set loss: 798.9906\n",
      "====> Epoch: 111 Average loss: 786.2510\n",
      "====> Test set loss: 797.2904\n",
      "====> Epoch: 112 Average loss: 786.2877\n",
      "====> Test set loss: 797.6992\n",
      "====> Epoch: 113 Average loss: 786.3691\n",
      "====> Test set loss: 798.4153\n",
      "====> Epoch: 114 Average loss: 785.9187\n",
      "====> Test set loss: 797.8935\n",
      "====> Epoch: 115 Average loss: 786.2567\n",
      "====> Test set loss: 797.9451\n",
      "====> Epoch: 116 Average loss: 785.8272\n",
      "====> Test set loss: 797.3873\n",
      "====> Epoch: 117 Average loss: 786.0625\n",
      "====> Test set loss: 796.6982\n",
      "====> Epoch: 118 Average loss: 785.5675\n",
      "====> Test set loss: 797.0160\n",
      "====> Epoch: 119 Average loss: 785.2698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 797.5992\n",
      "====> Epoch: 120 Average loss: 785.3688\n",
      "====> Test set loss: 798.2648\n",
      "====> Epoch: 121 Average loss: 785.0113\n",
      "====> Test set loss: 796.8887\n",
      "====> Epoch: 122 Average loss: 785.3412\n",
      "====> Test set loss: 797.0159\n",
      "====> Epoch: 123 Average loss: 785.4243\n",
      "====> Test set loss: 798.9056\n",
      "====> Epoch: 124 Average loss: 784.9404\n",
      "====> Test set loss: 798.4592\n",
      "====> Epoch: 125 Average loss: 784.5412\n",
      "====> Test set loss: 798.3335\n",
      "====> Epoch: 126 Average loss: 784.4241\n",
      "====> Test set loss: 797.3208\n",
      "====> Epoch: 127 Average loss: 784.6552\n",
      "====> Test set loss: 797.3448\n",
      "====> Epoch: 128 Average loss: 784.6793\n",
      "====> Test set loss: 796.3907\n",
      "====> Epoch: 129 Average loss: 784.2438\n",
      "====> Test set loss: 797.2036\n",
      "====> Epoch: 130 Average loss: 784.4245\n",
      "====> Test set loss: 798.3585\n",
      "====> Epoch: 131 Average loss: 784.3349\n",
      "====> Test set loss: 795.9591\n",
      "====> Epoch: 132 Average loss: 783.8246\n",
      "====> Test set loss: 796.5787\n",
      "====> Epoch: 133 Average loss: 783.7961\n",
      "====> Test set loss: 797.1942\n",
      "====> Epoch: 134 Average loss: 783.5497\n",
      "====> Test set loss: 797.1686\n",
      "====> Epoch: 135 Average loss: 783.5629\n",
      "====> Test set loss: 798.4408\n",
      "====> Epoch: 136 Average loss: 783.8210\n",
      "====> Test set loss: 798.0084\n",
      "====> Epoch: 137 Average loss: 783.7631\n",
      "====> Test set loss: 796.4104\n",
      "====> Epoch: 138 Average loss: 783.4142\n",
      "====> Test set loss: 796.7036\n",
      "====> Epoch: 139 Average loss: 783.5998\n",
      "====> Test set loss: 796.6497\n",
      "====> Epoch: 140 Average loss: 783.3508\n",
      "====> Test set loss: 796.7502\n",
      "====> Epoch: 141 Average loss: 783.2647\n",
      "====> Test set loss: 796.3809\n",
      "====> Epoch: 142 Average loss: 783.0407\n",
      "====> Test set loss: 797.3242\n",
      "====> Epoch: 143 Average loss: 783.1778\n",
      "====> Test set loss: 797.2306\n",
      "====> Epoch: 144 Average loss: 783.1191\n",
      "====> Test set loss: 795.9550\n",
      "====> Epoch: 145 Average loss: 782.7104\n",
      "====> Test set loss: 796.7922\n",
      "====> Epoch: 146 Average loss: 782.5780\n",
      "====> Test set loss: 796.3067\n",
      "====> Epoch: 147 Average loss: 782.6939\n",
      "====> Test set loss: 795.5575\n",
      "====> Epoch: 148 Average loss: 782.6251\n",
      "====> Test set loss: 795.6711\n",
      "====> Epoch: 149 Average loss: 782.4775\n",
      "====> Test set loss: 796.1569\n",
      "====> Epoch: 150 Average loss: 782.5496\n",
      "====> Test set loss: 796.0731\n",
      "====> Epoch: 151 Average loss: 782.5163\n",
      "====> Test set loss: 796.8983\n",
      "====> Epoch: 152 Average loss: 782.5455\n",
      "====> Test set loss: 798.3880\n",
      "====> Epoch: 153 Average loss: 782.1746\n",
      "====> Test set loss: 796.7633\n",
      "====> Epoch: 154 Average loss: 782.0023\n",
      "====> Test set loss: 796.4373\n",
      "====> Epoch: 155 Average loss: 782.0289\n",
      "====> Test set loss: 795.3713\n",
      "====> Epoch: 156 Average loss: 782.0060\n",
      "====> Test set loss: 795.8616\n",
      "====> Epoch: 157 Average loss: 781.6570\n",
      "====> Test set loss: 796.6759\n",
      "====> Epoch: 158 Average loss: 781.7269\n",
      "====> Test set loss: 796.0395\n",
      "====> Epoch: 159 Average loss: 781.5833\n",
      "====> Test set loss: 795.7897\n",
      "====> Epoch: 160 Average loss: 781.5213\n",
      "====> Test set loss: 795.8033\n",
      "====> Epoch: 161 Average loss: 781.1064\n",
      "====> Test set loss: 796.9461\n",
      "====> Epoch: 162 Average loss: 780.9083\n",
      "====> Test set loss: 796.5546\n",
      "====> Epoch: 163 Average loss: 781.1288\n",
      "====> Test set loss: 796.3721\n",
      "====> Epoch: 164 Average loss: 781.5340\n",
      "====> Test set loss: 795.4505\n",
      "====> Epoch: 165 Average loss: 781.0106\n",
      "====> Test set loss: 794.9607\n",
      "====> Epoch: 166 Average loss: 781.6699\n",
      "====> Test set loss: 794.8978\n",
      "====> Epoch: 167 Average loss: 781.3151\n",
      "====> Test set loss: 794.6752\n",
      "====> Epoch: 168 Average loss: 781.1534\n",
      "====> Test set loss: 795.6150\n",
      "====> Epoch: 169 Average loss: 781.0857\n",
      "====> Test set loss: 795.4705\n",
      "====> Epoch: 170 Average loss: 781.0002\n",
      "====> Test set loss: 794.7871\n",
      "====> Epoch: 171 Average loss: 780.9153\n",
      "====> Test set loss: 794.4033\n",
      "====> Epoch: 172 Average loss: 780.4155\n",
      "====> Test set loss: 794.8229\n",
      "====> Epoch: 173 Average loss: 780.6916\n",
      "====> Test set loss: 794.4808\n",
      "====> Epoch: 174 Average loss: 780.8588\n",
      "====> Test set loss: 795.1648\n",
      "====> Epoch: 175 Average loss: 780.6163\n",
      "====> Test set loss: 796.2872\n",
      "====> Epoch: 176 Average loss: 780.6502\n",
      "====> Test set loss: 795.9385\n",
      "====> Epoch: 177 Average loss: 780.2665\n",
      "====> Test set loss: 795.0775\n",
      "====> Epoch: 178 Average loss: 779.7974\n",
      "====> Test set loss: 795.7452\n",
      "====> Epoch: 179 Average loss: 780.5746\n",
      "====> Test set loss: 793.6366\n",
      "====> Epoch: 180 Average loss: 780.2143\n",
      "====> Test set loss: 796.2530\n",
      "====> Epoch: 181 Average loss: 780.1694\n",
      "====> Test set loss: 795.3498\n",
      "====> Epoch: 182 Average loss: 780.1666\n",
      "====> Test set loss: 795.1241\n",
      "====> Epoch: 183 Average loss: 780.2807\n",
      "====> Test set loss: 796.7900\n",
      "====> Epoch: 184 Average loss: 780.0482\n",
      "====> Test set loss: 795.5752\n",
      "====> Epoch: 185 Average loss: 780.0468\n",
      "====> Test set loss: 795.7641\n",
      "====> Epoch: 186 Average loss: 779.7403\n",
      "====> Test set loss: 795.7474\n",
      "====> Epoch: 187 Average loss: 779.9492\n",
      "====> Test set loss: 794.9327\n",
      "====> Epoch: 188 Average loss: 779.6945\n",
      "====> Test set loss: 796.4903\n",
      "====> Epoch: 189 Average loss: 780.0075\n",
      "====> Test set loss: 795.2438\n",
      "====> Epoch: 190 Average loss: 779.7735\n",
      "====> Test set loss: 795.0433\n",
      "====> Epoch: 191 Average loss: 779.4818\n",
      "====> Test set loss: 795.2665\n",
      "====> Epoch: 192 Average loss: 779.9638\n",
      "====> Test set loss: 796.3515\n",
      "====> Epoch: 193 Average loss: 779.5726\n",
      "====> Test set loss: 796.3178\n",
      "====> Epoch: 194 Average loss: 779.3669\n",
      "====> Test set loss: 796.0278\n",
      "====> Epoch: 195 Average loss: 779.5691\n",
      "====> Test set loss: 796.1189\n",
      "====> Epoch: 196 Average loss: 779.8234\n",
      "====> Test set loss: 795.5228\n",
      "====> Epoch: 197 Average loss: 779.4974\n",
      "====> Test set loss: 795.0047\n",
      "====> Epoch: 198 Average loss: 779.4361\n",
      "====> Test set loss: 795.1366\n",
      "====> Epoch: 199 Average loss: 778.9597\n",
      "====> Test set loss: 794.6415\n",
      "====> Epoch: 200 Average loss: 779.3778\n",
      "====> Test set loss: 796.8870\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    with torch.no_grad():\n",
    "        if(epoch > epochs - 10):\n",
    "            sample = torch.randn(64, l_d).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRDJKASAiklH/H3ZFDQwnTa7oAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(train_losses),'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(test_losses),'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
