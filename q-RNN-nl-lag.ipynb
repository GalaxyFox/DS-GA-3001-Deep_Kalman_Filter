{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d59522e530>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\"\"\"\n",
    "batch_size = 128\n",
    "seed = 1\n",
    "epochs = 300\n",
    "cuda = True\n",
    "log_interval = 10\n",
    "sample_size = 10\n",
    "h_d = 512\n",
    "l_d = 32\n",
    "u_d = 1\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hmnist dataset\n",
    "import healing_mnist_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hmnist = healing_mnist_penalty.HealingMNIST(seq_len=5, # 5 rotations of each digit\n",
    "                                          square_count=0, # 3 out of 5 images have a square added to them\n",
    "                                          square_size=5, # the square is 5x5\n",
    "                                          noise_ratio=0.1, # on average, 20% of the image is eaten by noise,\n",
    "                                          digits=range(10), # only include this digits\n",
    "                                          test = False,\n",
    "                                          lag = 0.3\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5, 28, 28)\n",
      "(60000, 5)\n",
      "(10000, 5, 28, 28) (10000, 5, 28, 28)\n",
      "(10000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(hmnist.train_images.shape,hmnist.train_targets.shape)\n",
    "print(hmnist.train_rotations.shape)\n",
    "print(hmnist.test_images.shape,hmnist.test_targets.shape)\n",
    "print(hmnist.test_rotations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtsAAACaCAYAAAB464RIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1080x576 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "case = 258\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "for i, image in enumerate(hmnist.test_images[case]):\n",
    "    fig.add_subplot(1, 6, i+1)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtsAAACaCAYAAAB464RIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsw0AAJ1otgEAoBPNNgAAdPK/mPzrRLyKmDcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x576 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "for i, image in enumerate(hmnist.test_targets[case]):\n",
    "    fig.add_subplot(1, 6, i+1)\n",
    "    plt.imshow(image,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-83.26112369  20.41486718  66.59547701  86.82523723  31.94632837]\n"
     ]
    }
   ],
   "source": [
    "print(hmnist.test_rotations[case])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the data shape\n",
    "this part should be different for different model, the q-RNN model does not igorned the sequencial dependency within the dataset, so we don't need to flat the dataset. So that the dataset should be that given a sequence of noisy image $\\{p_i\\}$ and a sequences of action $\\{u_i\\}$, the target should be the image of next timestep given action $u_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5) (60000, 5, 28, 28)\n",
      "(10000, 5, 28, 28) (10000, 5) (10000, 5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "train_X = hmnist.train_images\n",
    "train_u = hmnist.train_rotations\n",
    "train_Y = hmnist.train_targets\n",
    "test_X = hmnist.test_images\n",
    "test_u = hmnist.test_rotations\n",
    "test_Y = hmnist.test_targets\n",
    "print(train_X.shape,train_u.shape,train_Y.shape)\n",
    "print(test_X.shape,test_u.shape,test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5) (60000, 5, 28, 28)\n",
      "(10000, 5, 28, 28) (10000, 5) (10000, 5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#no adjustment needed\n",
    "\n",
    "#train_Y = train_Y[:,4,:,:]\n",
    "#test_Y = test_Y[:,4,:,:]\n",
    "\n",
    "print(train_X.shape,train_u.shape,train_Y.shape)\n",
    "print(test_X.shape,test_u.shape,test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HMNISTDataSet():\n",
    "    def __init__(self, train_img, train_act, train_tar, test_img, test_act, test_tar, test = False, transform=None):\n",
    "        self.test = test\n",
    "        self.transform = transform\n",
    "\n",
    "        if (self.test == False):\n",
    "          self.images = train_img\n",
    "          self.targets = train_tar\n",
    "          self.rotations = train_act\n",
    "\n",
    "        else:      \n",
    "          self.images = test_img\n",
    "          self.targets = test_tar\n",
    "          self.rotations = test_act\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform is not None:\n",
    "            img = torch.zeros((len(self.images[index]),1,28,28))\n",
    "            for i in range(len(self.images[index])):\n",
    "                img[i] = self.transform(self.images[index][i].reshape(28,28,1))\n",
    "            tar = torch.zeros((len(self.targets[index]),1,28,28))\n",
    "            for i in range(len(self.targets[index])):\n",
    "                tar[i] = self.transform(self.targets[index][i].reshape(28,28,1))\n",
    "                \n",
    "            rot = torch.tensor(self.rotations[index])\n",
    "        return img, rot, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = HMNISTDataSet(train_X, train_u, train_Y, test_X, test_u, test_Y, test = False, transform = transforms.ToTensor())\n",
    "test_set = HMNISTDataSet(train_X, train_u, train_Y, test_X, test_u, test_Y, test = True, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 28, 28])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c = train_set.__getitem__(3)\n",
    "c.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, h_d)\n",
    "        self.fc2 = nn.Linear(h_d,128)\n",
    "        self.fc21 = nn.Linear(128, l_d)\n",
    "        self.fc22 = nn.Linear(128, l_d)\n",
    "        \n",
    "        #transition layer\n",
    "        input_dim = l_d + u_d\n",
    "        self.rnn_mu = nn.RNN(input_size=input_dim,hidden_size=l_d,batch_first=True)\n",
    "        self.rnn_sigma = nn.RNN(input_size=input_dim,hidden_size=l_d,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(l_d, 128)\n",
    "        self.fc4 = nn.Linear(128,h_d)\n",
    "        self.fc5 = nn.Linear(h_d, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc21(h2), self.fc22(h2)\n",
    "\n",
    "    def reparameterize1(self, mu, logvar, n=1):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def transition(self, z, u):\n",
    "        rnn_input = torch.cat((z,u),dim=2)\n",
    "        mu2,_ = self.rnn_mu(rnn_input)\n",
    "        logvar2,_ = self.rnn_sigma(rnn_input)\n",
    "        return mu2,logvar2\n",
    "    \n",
    "    def reparameterize2(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        h4 = F.relu(self.fc4(h3))\n",
    "        return torch.sigmoid(self.fc5(h4))\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        mu1, logvar1 = self.encode(x.view(-1, 784))\n",
    "        mu = torch.empty(sample_size, x.shape[0], x.shape[1], l_d , dtype=torch.float).to(device)\n",
    "        logvar = torch.empty(sample_size, x.shape[0], x.shape[1], l_d , dtype=torch.float).to(device)\n",
    "        for i in range(sample_size):\n",
    "            z1 = self.reparameterize1(mu1, logvar1)\n",
    "            z1 = z1.reshape(-1,5,32)\n",
    "            u = u.float()\n",
    "            mu2, logvar2 = self.transition(z1,u.reshape(-1,5,1))\n",
    "            mu[i] = mu2\n",
    "            logvar[i] = logvar2\n",
    "        z2 = self.reparameterize2(mu[0], logvar[0])\n",
    "        \n",
    "        return self.decode(z2), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "#adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar[0,:,0,:] - mu[0,:,0,:].pow(2) - logvar[0,:,0,:].exp())\n",
    "    EKLD = 0\n",
    "    for i in range(1,5):        \n",
    "        tmp = -0.5 * torch.sum(1 + logvar[:,:,i,:] - mu[:,:,i,:].pow(2) - logvar[:,:,i,:].exp())/sample_size\n",
    "        EKLD += tmp\n",
    "    return BCE + KLD + EKLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (img, action, target) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        action = action.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(img,action)\n",
    "        loss = loss_function(recon_batch, target, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        #if batch_idx % log_interval == 0:\n",
    "         #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          #      epoch, batch_idx * len(img), len(train_loader.dataset),\n",
    "           #     100. * batch_idx / len(train_loader),\n",
    "            #    loss.item() / len(img)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (img, action, target) in enumerate(test_loader):\n",
    "            img = img.to(device)\n",
    "            action = action.to(device)\n",
    "            target = target.to(device)\n",
    "            recon_batch, mu, logvar = model(img, action)\n",
    "            test_loss += loss_function(recon_batch, target, mu, logvar).item()\n",
    "            if(epoch > epochs - 10):\n",
    "                if i == 0:\n",
    "                    n = np.random.randint(0,batch_size)\n",
    "                    comparison = torch.cat([target[n],\n",
    "                                          recon_batch.view(batch_size, 5, 1, 28, 28)[n]],dim=0)\n",
    "                    save_image(comparison.cpu(),\n",
    "                             'results_rnn_lag/reconstruction_' + str(epoch) + '.png', nrow=5)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 1106.5355\n",
      "====> Test set loss: 1030.8732\n",
      "====> Epoch: 2 Average loss: 1027.0000\n",
      "====> Test set loss: 1021.9636\n",
      "====> Epoch: 3 Average loss: 1019.2032\n",
      "====> Test set loss: 1011.6198\n",
      "====> Epoch: 4 Average loss: 987.5400\n",
      "====> Test set loss: 949.3501\n",
      "====> Epoch: 5 Average loss: 934.5039\n",
      "====> Test set loss: 918.4425\n",
      "====> Epoch: 6 Average loss: 910.5026\n",
      "====> Test set loss: 899.2314\n",
      "====> Epoch: 7 Average loss: 893.2691\n",
      "====> Test set loss: 886.1238\n",
      "====> Epoch: 8 Average loss: 880.0682\n",
      "====> Test set loss: 873.9704\n",
      "====> Epoch: 9 Average loss: 870.5317\n",
      "====> Test set loss: 864.4498\n",
      "====> Epoch: 10 Average loss: 863.0840\n",
      "====> Test set loss: 858.1905\n",
      "====> Epoch: 11 Average loss: 856.9030\n",
      "====> Test set loss: 856.4939\n",
      "====> Epoch: 12 Average loss: 851.7836\n",
      "====> Test set loss: 848.7369\n",
      "====> Epoch: 13 Average loss: 847.3081\n",
      "====> Test set loss: 845.4830\n",
      "====> Epoch: 14 Average loss: 843.8990\n",
      "====> Test set loss: 842.8649\n",
      "====> Epoch: 15 Average loss: 840.6223\n",
      "====> Test set loss: 840.6025\n",
      "====> Epoch: 16 Average loss: 836.9991\n",
      "====> Test set loss: 836.6793\n",
      "====> Epoch: 17 Average loss: 834.2975\n",
      "====> Test set loss: 833.4335\n",
      "====> Epoch: 18 Average loss: 832.2252\n",
      "====> Test set loss: 833.6152\n",
      "====> Epoch: 19 Average loss: 829.2263\n",
      "====> Test set loss: 832.4159\n",
      "====> Epoch: 20 Average loss: 827.3422\n",
      "====> Test set loss: 830.3748\n",
      "====> Epoch: 21 Average loss: 825.9602\n",
      "====> Test set loss: 828.9867\n",
      "====> Epoch: 22 Average loss: 823.9132\n",
      "====> Test set loss: 826.2262\n",
      "====> Epoch: 23 Average loss: 821.8732\n",
      "====> Test set loss: 826.0304\n",
      "====> Epoch: 24 Average loss: 820.6726\n",
      "====> Test set loss: 825.0114\n",
      "====> Epoch: 25 Average loss: 819.4920\n",
      "====> Test set loss: 823.2499\n",
      "====> Epoch: 26 Average loss: 817.9635\n",
      "====> Test set loss: 822.5709\n",
      "====> Epoch: 27 Average loss: 816.4649\n",
      "====> Test set loss: 821.6394\n",
      "====> Epoch: 28 Average loss: 815.5811\n",
      "====> Test set loss: 820.5435\n",
      "====> Epoch: 29 Average loss: 814.1146\n",
      "====> Test set loss: 819.9280\n",
      "====> Epoch: 30 Average loss: 813.2895\n",
      "====> Test set loss: 820.5726\n",
      "====> Epoch: 31 Average loss: 812.0205\n",
      "====> Test set loss: 818.2836\n",
      "====> Epoch: 32 Average loss: 810.9223\n",
      "====> Test set loss: 816.3244\n",
      "====> Epoch: 33 Average loss: 809.4464\n",
      "====> Test set loss: 816.1185\n",
      "====> Epoch: 34 Average loss: 808.4870\n",
      "====> Test set loss: 816.1159\n",
      "====> Epoch: 35 Average loss: 807.6159\n",
      "====> Test set loss: 815.1846\n",
      "====> Epoch: 36 Average loss: 806.7606\n",
      "====> Test set loss: 815.3828\n",
      "====> Epoch: 37 Average loss: 805.5940\n",
      "====> Test set loss: 815.9438\n",
      "====> Epoch: 38 Average loss: 805.2694\n",
      "====> Test set loss: 813.9883\n",
      "====> Epoch: 39 Average loss: 803.8845\n",
      "====> Test set loss: 814.2221\n",
      "====> Epoch: 40 Average loss: 803.4115\n",
      "====> Test set loss: 810.3123\n",
      "====> Epoch: 41 Average loss: 802.2235\n",
      "====> Test set loss: 811.4488\n",
      "====> Epoch: 42 Average loss: 801.6988\n",
      "====> Test set loss: 811.7668\n",
      "====> Epoch: 43 Average loss: 801.1218\n",
      "====> Test set loss: 810.6715\n",
      "====> Epoch: 44 Average loss: 800.4898\n",
      "====> Test set loss: 808.8951\n",
      "====> Epoch: 45 Average loss: 799.2360\n",
      "====> Test set loss: 808.3745\n",
      "====> Epoch: 46 Average loss: 798.6720\n",
      "====> Test set loss: 809.2889\n",
      "====> Epoch: 47 Average loss: 797.9543\n",
      "====> Test set loss: 808.6587\n",
      "====> Epoch: 48 Average loss: 797.7378\n",
      "====> Test set loss: 806.7121\n",
      "====> Epoch: 49 Average loss: 797.5959\n",
      "====> Test set loss: 807.9224\n",
      "====> Epoch: 50 Average loss: 796.7476\n",
      "====> Test set loss: 808.6426\n",
      "====> Epoch: 51 Average loss: 796.1129\n",
      "====> Test set loss: 805.8414\n",
      "====> Epoch: 52 Average loss: 795.1893\n",
      "====> Test set loss: 805.7225\n",
      "====> Epoch: 53 Average loss: 795.0321\n",
      "====> Test set loss: 806.0515\n",
      "====> Epoch: 54 Average loss: 794.5241\n",
      "====> Test set loss: 805.5458\n",
      "====> Epoch: 55 Average loss: 794.1525\n",
      "====> Test set loss: 806.0138\n",
      "====> Epoch: 56 Average loss: 792.9761\n",
      "====> Test set loss: 807.4720\n",
      "====> Epoch: 57 Average loss: 793.5814\n",
      "====> Test set loss: 805.1649\n",
      "====> Epoch: 58 Average loss: 793.0337\n",
      "====> Test set loss: 806.3792\n",
      "====> Epoch: 59 Average loss: 792.3990\n",
      "====> Test set loss: 806.0853\n",
      "====> Epoch: 60 Average loss: 791.9842\n",
      "====> Test set loss: 803.7842\n",
      "====> Epoch: 61 Average loss: 791.4672\n",
      "====> Test set loss: 803.8425\n",
      "====> Epoch: 62 Average loss: 791.8290\n",
      "====> Test set loss: 806.0548\n",
      "====> Epoch: 63 Average loss: 791.2073\n",
      "====> Test set loss: 804.2075\n",
      "====> Epoch: 64 Average loss: 790.3996\n",
      "====> Test set loss: 804.1501\n",
      "====> Epoch: 65 Average loss: 790.3972\n",
      "====> Test set loss: 804.1977\n",
      "====> Epoch: 66 Average loss: 790.1692\n",
      "====> Test set loss: 804.2763\n",
      "====> Epoch: 67 Average loss: 789.8586\n",
      "====> Test set loss: 803.5639\n",
      "====> Epoch: 68 Average loss: 789.0552\n",
      "====> Test set loss: 804.3715\n",
      "====> Epoch: 69 Average loss: 789.3117\n",
      "====> Test set loss: 803.6554\n",
      "====> Epoch: 70 Average loss: 788.2768\n",
      "====> Test set loss: 802.2165\n",
      "====> Epoch: 71 Average loss: 788.2950\n",
      "====> Test set loss: 801.5498\n",
      "====> Epoch: 72 Average loss: 788.0258\n",
      "====> Test set loss: 803.2371\n",
      "====> Epoch: 73 Average loss: 787.4260\n",
      "====> Test set loss: 802.7359\n",
      "====> Epoch: 74 Average loss: 787.3438\n",
      "====> Test set loss: 804.1442\n",
      "====> Epoch: 75 Average loss: 787.6635\n",
      "====> Test set loss: 803.4061\n",
      "====> Epoch: 76 Average loss: 786.8046\n",
      "====> Test set loss: 802.7108\n",
      "====> Epoch: 77 Average loss: 786.2657\n",
      "====> Test set loss: 803.1093\n",
      "====> Epoch: 78 Average loss: 786.7353\n",
      "====> Test set loss: 803.6231\n",
      "====> Epoch: 79 Average loss: 786.2550\n",
      "====> Test set loss: 802.5501\n",
      "====> Epoch: 80 Average loss: 785.6847\n",
      "====> Test set loss: 802.3125\n",
      "====> Epoch: 81 Average loss: 785.2329\n",
      "====> Test set loss: 803.4083\n",
      "====> Epoch: 82 Average loss: 785.6466\n",
      "====> Test set loss: 803.9785\n",
      "====> Epoch: 83 Average loss: 785.4221\n",
      "====> Test set loss: 802.4506\n",
      "====> Epoch: 84 Average loss: 784.8961\n",
      "====> Test set loss: 801.7104\n",
      "====> Epoch: 85 Average loss: 784.9222\n",
      "====> Test set loss: 803.8605\n",
      "====> Epoch: 86 Average loss: 784.4326\n",
      "====> Test set loss: 801.2370\n",
      "====> Epoch: 87 Average loss: 784.0274\n",
      "====> Test set loss: 801.4381\n",
      "====> Epoch: 88 Average loss: 784.6409\n",
      "====> Test set loss: 801.8844\n",
      "====> Epoch: 89 Average loss: 784.6230\n",
      "====> Test set loss: 801.4455\n",
      "====> Epoch: 90 Average loss: 784.0573\n",
      "====> Test set loss: 801.4541\n",
      "====> Epoch: 91 Average loss: 783.7751\n",
      "====> Test set loss: 801.9221\n",
      "====> Epoch: 92 Average loss: 782.9869\n",
      "====> Test set loss: 801.9708\n",
      "====> Epoch: 93 Average loss: 783.0060\n",
      "====> Test set loss: 800.5810\n",
      "====> Epoch: 94 Average loss: 783.2136\n",
      "====> Test set loss: 801.8774\n",
      "====> Epoch: 95 Average loss: 783.3806\n",
      "====> Test set loss: 803.7585\n",
      "====> Epoch: 96 Average loss: 783.1732\n",
      "====> Test set loss: 802.3753\n",
      "====> Epoch: 97 Average loss: 782.6521\n",
      "====> Test set loss: 802.2646\n",
      "====> Epoch: 98 Average loss: 782.6575\n",
      "====> Test set loss: 800.4196\n",
      "====> Epoch: 99 Average loss: 782.4723\n",
      "====> Test set loss: 801.1700\n",
      "====> Epoch: 100 Average loss: 782.1624\n",
      "====> Test set loss: 801.8934\n",
      "====> Epoch: 101 Average loss: 781.9661\n",
      "====> Test set loss: 802.7204\n",
      "====> Epoch: 102 Average loss: 781.7272\n",
      "====> Test set loss: 801.5989\n",
      "====> Epoch: 103 Average loss: 782.0666\n",
      "====> Test set loss: 801.0384\n",
      "====> Epoch: 104 Average loss: 781.9796\n",
      "====> Test set loss: 800.9938\n",
      "====> Epoch: 105 Average loss: 781.4939\n",
      "====> Test set loss: 801.8989\n",
      "====> Epoch: 106 Average loss: 781.5046\n",
      "====> Test set loss: 800.3538\n",
      "====> Epoch: 107 Average loss: 781.1575\n",
      "====> Test set loss: 801.2571\n",
      "====> Epoch: 108 Average loss: 781.2295\n",
      "====> Test set loss: 802.4358\n",
      "====> Epoch: 109 Average loss: 780.7688\n",
      "====> Test set loss: 801.0894\n",
      "====> Epoch: 110 Average loss: 780.7312\n",
      "====> Test set loss: 804.2044\n",
      "====> Epoch: 111 Average loss: 780.5081\n",
      "====> Test set loss: 801.9904\n",
      "====> Epoch: 112 Average loss: 779.9991\n",
      "====> Test set loss: 800.6097\n",
      "====> Epoch: 113 Average loss: 780.0478\n",
      "====> Test set loss: 802.2527\n",
      "====> Epoch: 114 Average loss: 780.3876\n",
      "====> Test set loss: 800.9525\n",
      "====> Epoch: 115 Average loss: 780.3917\n",
      "====> Test set loss: 800.1809\n",
      "====> Epoch: 116 Average loss: 779.9378\n",
      "====> Test set loss: 801.3384\n",
      "====> Epoch: 117 Average loss: 779.9827\n",
      "====> Test set loss: 799.3397\n",
      "====> Epoch: 118 Average loss: 779.7865\n",
      "====> Test set loss: 802.3565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 119 Average loss: 779.8863\n",
      "====> Test set loss: 800.3765\n",
      "====> Epoch: 120 Average loss: 779.5452\n",
      "====> Test set loss: 800.8060\n",
      "====> Epoch: 121 Average loss: 779.2909\n",
      "====> Test set loss: 800.6118\n",
      "====> Epoch: 122 Average loss: 778.8343\n",
      "====> Test set loss: 801.1406\n",
      "====> Epoch: 123 Average loss: 779.2788\n",
      "====> Test set loss: 801.5384\n",
      "====> Epoch: 124 Average loss: 778.3202\n",
      "====> Test set loss: 802.3135\n",
      "====> Epoch: 125 Average loss: 778.7440\n",
      "====> Test set loss: 800.1767\n",
      "====> Epoch: 126 Average loss: 779.0427\n",
      "====> Test set loss: 802.1408\n",
      "====> Epoch: 127 Average loss: 778.6820\n",
      "====> Test set loss: 802.2211\n",
      "====> Epoch: 128 Average loss: 779.3442\n",
      "====> Test set loss: 802.4049\n",
      "====> Epoch: 129 Average loss: 778.5946\n",
      "====> Test set loss: 800.7817\n",
      "====> Epoch: 130 Average loss: 778.6602\n",
      "====> Test set loss: 800.6232\n",
      "====> Epoch: 131 Average loss: 778.0855\n",
      "====> Test set loss: 799.9999\n",
      "====> Epoch: 132 Average loss: 778.2087\n",
      "====> Test set loss: 802.4366\n",
      "====> Epoch: 133 Average loss: 777.9244\n",
      "====> Test set loss: 802.5392\n",
      "====> Epoch: 134 Average loss: 778.0966\n",
      "====> Test set loss: 802.3914\n",
      "====> Epoch: 135 Average loss: 777.8831\n",
      "====> Test set loss: 801.3034\n",
      "====> Epoch: 136 Average loss: 777.7261\n",
      "====> Test set loss: 800.2066\n",
      "====> Epoch: 137 Average loss: 777.4365\n",
      "====> Test set loss: 799.8856\n",
      "====> Epoch: 138 Average loss: 777.9978\n",
      "====> Test set loss: 801.3973\n",
      "====> Epoch: 139 Average loss: 777.6904\n",
      "====> Test set loss: 801.8334\n",
      "====> Epoch: 140 Average loss: 777.5828\n",
      "====> Test set loss: 798.9988\n",
      "====> Epoch: 141 Average loss: 777.3934\n",
      "====> Test set loss: 800.9165\n",
      "====> Epoch: 142 Average loss: 777.1715\n",
      "====> Test set loss: 801.9450\n",
      "====> Epoch: 143 Average loss: 777.2203\n",
      "====> Test set loss: 799.2807\n",
      "====> Epoch: 144 Average loss: 776.5861\n",
      "====> Test set loss: 800.8732\n",
      "====> Epoch: 145 Average loss: 776.9350\n",
      "====> Test set loss: 800.7483\n",
      "====> Epoch: 146 Average loss: 777.0833\n",
      "====> Test set loss: 801.1948\n",
      "====> Epoch: 147 Average loss: 776.9876\n",
      "====> Test set loss: 801.7142\n",
      "====> Epoch: 148 Average loss: 777.3428\n",
      "====> Test set loss: 801.5582\n",
      "====> Epoch: 149 Average loss: 776.2870\n",
      "====> Test set loss: 800.0388\n",
      "====> Epoch: 150 Average loss: 776.2477\n",
      "====> Test set loss: 800.8844\n",
      "====> Epoch: 151 Average loss: 776.9893\n",
      "====> Test set loss: 802.2175\n",
      "====> Epoch: 152 Average loss: 776.2996\n",
      "====> Test set loss: 800.9474\n",
      "====> Epoch: 153 Average loss: 776.1048\n",
      "====> Test set loss: 798.9966\n",
      "====> Epoch: 154 Average loss: 776.4375\n",
      "====> Test set loss: 800.9590\n",
      "====> Epoch: 155 Average loss: 776.2819\n",
      "====> Test set loss: 800.3448\n",
      "====> Epoch: 156 Average loss: 776.5390\n",
      "====> Test set loss: 802.1449\n",
      "====> Epoch: 157 Average loss: 776.0561\n",
      "====> Test set loss: 801.9261\n",
      "====> Epoch: 158 Average loss: 776.2257\n",
      "====> Test set loss: 800.4345\n",
      "====> Epoch: 159 Average loss: 775.5440\n",
      "====> Test set loss: 801.8899\n",
      "====> Epoch: 160 Average loss: 775.5822\n",
      "====> Test set loss: 799.7024\n",
      "====> Epoch: 161 Average loss: 775.6693\n",
      "====> Test set loss: 802.6490\n",
      "====> Epoch: 162 Average loss: 775.6780\n",
      "====> Test set loss: 801.3682\n",
      "====> Epoch: 163 Average loss: 775.4119\n",
      "====> Test set loss: 801.5177\n",
      "====> Epoch: 164 Average loss: 775.3360\n",
      "====> Test set loss: 800.2535\n",
      "====> Epoch: 165 Average loss: 775.9073\n",
      "====> Test set loss: 800.1857\n",
      "====> Epoch: 166 Average loss: 775.2065\n",
      "====> Test set loss: 800.2502\n",
      "====> Epoch: 167 Average loss: 775.3816\n",
      "====> Test set loss: 801.0108\n",
      "====> Epoch: 168 Average loss: 775.0258\n",
      "====> Test set loss: 802.6345\n",
      "====> Epoch: 169 Average loss: 775.2796\n",
      "====> Test set loss: 801.3987\n",
      "====> Epoch: 170 Average loss: 775.3421\n",
      "====> Test set loss: 801.1976\n",
      "====> Epoch: 171 Average loss: 775.0603\n",
      "====> Test set loss: 801.3743\n",
      "====> Epoch: 172 Average loss: 774.7546\n",
      "====> Test set loss: 801.5242\n",
      "====> Epoch: 173 Average loss: 774.8195\n",
      "====> Test set loss: 801.0405\n",
      "====> Epoch: 174 Average loss: 774.6934\n",
      "====> Test set loss: 800.0962\n",
      "====> Epoch: 175 Average loss: 774.5864\n",
      "====> Test set loss: 802.9689\n",
      "====> Epoch: 176 Average loss: 774.4246\n",
      "====> Test set loss: 802.8970\n",
      "====> Epoch: 177 Average loss: 774.4725\n",
      "====> Test set loss: 801.0928\n",
      "====> Epoch: 178 Average loss: 774.4788\n",
      "====> Test set loss: 801.1421\n",
      "====> Epoch: 179 Average loss: 774.4887\n",
      "====> Test set loss: 802.6458\n",
      "====> Epoch: 180 Average loss: 774.8006\n",
      "====> Test set loss: 801.3865\n",
      "====> Epoch: 181 Average loss: 774.7142\n",
      "====> Test set loss: 802.4958\n",
      "====> Epoch: 182 Average loss: 774.0865\n",
      "====> Test set loss: 800.4369\n",
      "====> Epoch: 183 Average loss: 773.9936\n",
      "====> Test set loss: 800.3072\n",
      "====> Epoch: 184 Average loss: 774.1524\n",
      "====> Test set loss: 801.9149\n",
      "====> Epoch: 185 Average loss: 774.6472\n",
      "====> Test set loss: 801.5050\n",
      "====> Epoch: 186 Average loss: 773.9133\n",
      "====> Test set loss: 802.3003\n",
      "====> Epoch: 187 Average loss: 774.0163\n",
      "====> Test set loss: 802.4485\n",
      "====> Epoch: 188 Average loss: 773.6409\n",
      "====> Test set loss: 801.5766\n",
      "====> Epoch: 189 Average loss: 773.9492\n",
      "====> Test set loss: 800.6608\n",
      "====> Epoch: 190 Average loss: 772.9912\n",
      "====> Test set loss: 802.0891\n",
      "====> Epoch: 191 Average loss: 773.5754\n",
      "====> Test set loss: 800.5824\n",
      "====> Epoch: 192 Average loss: 773.8840\n",
      "====> Test set loss: 801.1766\n",
      "====> Epoch: 193 Average loss: 773.6994\n",
      "====> Test set loss: 800.9867\n",
      "====> Epoch: 194 Average loss: 773.9733\n",
      "====> Test set loss: 801.6078\n",
      "====> Epoch: 195 Average loss: 773.3569\n",
      "====> Test set loss: 802.5255\n",
      "====> Epoch: 196 Average loss: 773.9665\n",
      "====> Test set loss: 802.8639\n",
      "====> Epoch: 197 Average loss: 773.1866\n",
      "====> Test set loss: 803.0092\n",
      "====> Epoch: 198 Average loss: 774.2894\n",
      "====> Test set loss: 801.3606\n",
      "====> Epoch: 199 Average loss: 773.3881\n",
      "====> Test set loss: 801.1668\n",
      "====> Epoch: 200 Average loss: 773.1043\n",
      "====> Test set loss: 802.7276\n",
      "====> Epoch: 201 Average loss: 773.5326\n",
      "====> Test set loss: 802.2431\n",
      "====> Epoch: 202 Average loss: 773.4581\n",
      "====> Test set loss: 801.9194\n",
      "====> Epoch: 203 Average loss: 772.8306\n",
      "====> Test set loss: 801.7845\n",
      "====> Epoch: 204 Average loss: 773.1239\n",
      "====> Test set loss: 801.3626\n",
      "====> Epoch: 205 Average loss: 772.5952\n",
      "====> Test set loss: 800.4303\n",
      "====> Epoch: 206 Average loss: 772.7586\n",
      "====> Test set loss: 802.0093\n",
      "====> Epoch: 207 Average loss: 772.6924\n",
      "====> Test set loss: 801.8441\n",
      "====> Epoch: 208 Average loss: 772.7051\n",
      "====> Test set loss: 801.7912\n",
      "====> Epoch: 209 Average loss: 773.1203\n",
      "====> Test set loss: 801.6364\n",
      "====> Epoch: 210 Average loss: 772.9051\n",
      "====> Test set loss: 803.1339\n",
      "====> Epoch: 211 Average loss: 772.8892\n",
      "====> Test set loss: 802.9680\n",
      "====> Epoch: 212 Average loss: 772.6075\n",
      "====> Test set loss: 801.8934\n",
      "====> Epoch: 213 Average loss: 772.3104\n",
      "====> Test set loss: 802.4371\n",
      "====> Epoch: 214 Average loss: 772.6289\n",
      "====> Test set loss: 801.3823\n",
      "====> Epoch: 215 Average loss: 773.1965\n",
      "====> Test set loss: 802.5080\n",
      "====> Epoch: 216 Average loss: 772.3078\n",
      "====> Test set loss: 803.0519\n",
      "====> Epoch: 217 Average loss: 772.6706\n",
      "====> Test set loss: 801.0246\n",
      "====> Epoch: 218 Average loss: 773.0812\n",
      "====> Test set loss: 802.2607\n",
      "====> Epoch: 219 Average loss: 772.2150\n",
      "====> Test set loss: 802.5563\n",
      "====> Epoch: 220 Average loss: 772.4058\n",
      "====> Test set loss: 800.5891\n",
      "====> Epoch: 221 Average loss: 772.1543\n",
      "====> Test set loss: 802.1162\n",
      "====> Epoch: 222 Average loss: 771.9914\n",
      "====> Test set loss: 803.7971\n",
      "====> Epoch: 223 Average loss: 772.2002\n",
      "====> Test set loss: 802.2904\n",
      "====> Epoch: 224 Average loss: 772.4592\n",
      "====> Test set loss: 801.1978\n",
      "====> Epoch: 225 Average loss: 771.8147\n",
      "====> Test set loss: 803.4117\n",
      "====> Epoch: 226 Average loss: 771.6985\n",
      "====> Test set loss: 803.6529\n",
      "====> Epoch: 227 Average loss: 772.3454\n",
      "====> Test set loss: 801.9917\n",
      "====> Epoch: 228 Average loss: 772.0431\n",
      "====> Test set loss: 802.1286\n",
      "====> Epoch: 229 Average loss: 772.0377\n",
      "====> Test set loss: 802.2878\n",
      "====> Epoch: 230 Average loss: 772.0126\n",
      "====> Test set loss: 804.2335\n",
      "====> Epoch: 231 Average loss: 772.2331\n",
      "====> Test set loss: 801.3547\n",
      "====> Epoch: 232 Average loss: 772.3424\n",
      "====> Test set loss: 803.3716\n",
      "====> Epoch: 233 Average loss: 771.9262\n",
      "====> Test set loss: 800.9051\n",
      "====> Epoch: 234 Average loss: 771.4586\n",
      "====> Test set loss: 802.9550\n",
      "====> Epoch: 235 Average loss: 771.6655\n",
      "====> Test set loss: 800.1401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 236 Average loss: 771.3341\n",
      "====> Test set loss: 803.1132\n",
      "====> Epoch: 237 Average loss: 772.1389\n",
      "====> Test set loss: 803.0179\n",
      "====> Epoch: 238 Average loss: 771.2205\n",
      "====> Test set loss: 802.9604\n",
      "====> Epoch: 239 Average loss: 771.7049\n",
      "====> Test set loss: 803.4955\n",
      "====> Epoch: 240 Average loss: 771.9387\n",
      "====> Test set loss: 804.4945\n",
      "====> Epoch: 241 Average loss: 771.8969\n",
      "====> Test set loss: 803.5605\n",
      "====> Epoch: 242 Average loss: 770.9473\n",
      "====> Test set loss: 801.4198\n",
      "====> Epoch: 243 Average loss: 771.7388\n",
      "====> Test set loss: 802.5701\n",
      "====> Epoch: 244 Average loss: 771.8969\n",
      "====> Test set loss: 802.3497\n",
      "====> Epoch: 245 Average loss: 771.7220\n",
      "====> Test set loss: 804.4494\n",
      "====> Epoch: 246 Average loss: 771.0693\n",
      "====> Test set loss: 801.5292\n",
      "====> Epoch: 247 Average loss: 770.9580\n",
      "====> Test set loss: 801.9932\n",
      "====> Epoch: 248 Average loss: 771.3393\n",
      "====> Test set loss: 801.2003\n",
      "====> Epoch: 249 Average loss: 771.2801\n",
      "====> Test set loss: 800.3070\n",
      "====> Epoch: 250 Average loss: 771.3704\n",
      "====> Test set loss: 802.8684\n",
      "====> Epoch: 251 Average loss: 770.9182\n",
      "====> Test set loss: 801.0665\n",
      "====> Epoch: 252 Average loss: 771.3556\n",
      "====> Test set loss: 803.7784\n",
      "====> Epoch: 253 Average loss: 771.0231\n",
      "====> Test set loss: 803.2171\n",
      "====> Epoch: 254 Average loss: 770.8733\n",
      "====> Test set loss: 801.0426\n",
      "====> Epoch: 255 Average loss: 770.8191\n",
      "====> Test set loss: 804.1159\n",
      "====> Epoch: 256 Average loss: 771.1807\n",
      "====> Test set loss: 802.1294\n",
      "====> Epoch: 257 Average loss: 771.0161\n",
      "====> Test set loss: 803.1168\n",
      "====> Epoch: 258 Average loss: 770.8279\n",
      "====> Test set loss: 802.7606\n",
      "====> Epoch: 259 Average loss: 770.9057\n",
      "====> Test set loss: 802.9187\n",
      "====> Epoch: 260 Average loss: 771.5806\n",
      "====> Test set loss: 801.2094\n",
      "====> Epoch: 261 Average loss: 771.5355\n",
      "====> Test set loss: 802.2978\n",
      "====> Epoch: 262 Average loss: 771.2232\n",
      "====> Test set loss: 803.5561\n",
      "====> Epoch: 263 Average loss: 770.4058\n",
      "====> Test set loss: 801.6377\n",
      "====> Epoch: 264 Average loss: 770.8873\n",
      "====> Test set loss: 802.1042\n",
      "====> Epoch: 265 Average loss: 771.1779\n",
      "====> Test set loss: 803.2630\n",
      "====> Epoch: 266 Average loss: 770.7337\n",
      "====> Test set loss: 804.7723\n",
      "====> Epoch: 267 Average loss: 771.0830\n",
      "====> Test set loss: 803.0262\n",
      "====> Epoch: 268 Average loss: 770.3983\n",
      "====> Test set loss: 802.9070\n",
      "====> Epoch: 269 Average loss: 770.6339\n",
      "====> Test set loss: 804.2848\n",
      "====> Epoch: 270 Average loss: 771.0937\n",
      "====> Test set loss: 803.6023\n",
      "====> Epoch: 271 Average loss: 770.5507\n",
      "====> Test set loss: 802.4355\n",
      "====> Epoch: 272 Average loss: 770.6322\n",
      "====> Test set loss: 801.1300\n",
      "====> Epoch: 273 Average loss: 770.0589\n",
      "====> Test set loss: 803.3690\n",
      "====> Epoch: 274 Average loss: 770.3487\n",
      "====> Test set loss: 804.3884\n",
      "====> Epoch: 275 Average loss: 770.3543\n",
      "====> Test set loss: 802.8462\n",
      "====> Epoch: 276 Average loss: 770.7587\n",
      "====> Test set loss: 803.9492\n",
      "====> Epoch: 277 Average loss: 770.5549\n",
      "====> Test set loss: 804.0587\n",
      "====> Epoch: 278 Average loss: 770.7986\n",
      "====> Test set loss: 803.8768\n",
      "====> Epoch: 279 Average loss: 770.4095\n",
      "====> Test set loss: 802.1312\n",
      "====> Epoch: 280 Average loss: 770.2731\n",
      "====> Test set loss: 803.1373\n",
      "====> Epoch: 281 Average loss: 770.2164\n",
      "====> Test set loss: 802.4062\n",
      "====> Epoch: 282 Average loss: 770.4948\n",
      "====> Test set loss: 803.5258\n",
      "====> Epoch: 283 Average loss: 770.3415\n",
      "====> Test set loss: 804.1996\n",
      "====> Epoch: 284 Average loss: 770.1476\n",
      "====> Test set loss: 802.3301\n",
      "====> Epoch: 285 Average loss: 771.3158\n",
      "====> Test set loss: 802.0266\n",
      "====> Epoch: 286 Average loss: 770.1503\n",
      "====> Test set loss: 805.3219\n",
      "====> Epoch: 287 Average loss: 769.9271\n",
      "====> Test set loss: 803.1119\n",
      "====> Epoch: 288 Average loss: 770.1143\n",
      "====> Test set loss: 802.9501\n",
      "====> Epoch: 289 Average loss: 769.9605\n",
      "====> Test set loss: 803.4292\n",
      "====> Epoch: 290 Average loss: 769.5640\n",
      "====> Test set loss: 802.3326\n",
      "====> Epoch: 291 Average loss: 769.5610\n",
      "====> Test set loss: 802.6844\n",
      "====> Epoch: 292 Average loss: 770.0451\n",
      "====> Test set loss: 803.3737\n",
      "====> Epoch: 293 Average loss: 769.8536\n",
      "====> Test set loss: 803.4499\n",
      "====> Epoch: 294 Average loss: 770.3066\n",
      "====> Test set loss: 803.3931\n",
      "====> Epoch: 295 Average loss: 770.3040\n",
      "====> Test set loss: 804.0895\n",
      "====> Epoch: 296 Average loss: 769.9161\n",
      "====> Test set loss: 806.6800\n",
      "====> Epoch: 297 Average loss: 769.7604\n",
      "====> Test set loss: 801.9965\n",
      "====> Epoch: 298 Average loss: 769.8108\n",
      "====> Test set loss: 802.7528\n",
      "====> Epoch: 299 Average loss: 769.8618\n",
      "====> Test set loss: 805.4181\n",
      "====> Epoch: 300 Average loss: 770.5256\n",
      "====> Test set loss: 804.2387\n"
     ]
    }
   ],
   "source": [
    "import warnings;\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    with torch.no_grad():\n",
    "        if(epoch > epochs - 10):\n",
    "            sample = torch.randn(64, l_d).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results_rnn_lag/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEDCAYAAAAyZm/jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwW9mVmf+P8Q8ZsTQmMtkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(train_losses),'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nACIiGfX/AY1ReGcScaNBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(test_losses),'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
