{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f7a393e4d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\"\"\"\n",
    "batch_size = 128\n",
    "seed = 1\n",
    "epochs = 200\n",
    "cuda = True\n",
    "log_interval = 10\n",
    "h_d = 512\n",
    "l_d = 32\n",
    "u_d = 1\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hmnist dataset\n",
    "import healing_mnist_indep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hmnist = healing_mnist_indep.HealingMNIST(seq_len=5, # 5 rotations of each digit\n",
    "                                          square_count=0, # 3 out of 5 images have a square added to them\n",
    "                                          square_size=5, # the square is 5x5\n",
    "                                          noise_ratio=0.10, # on average, 20% of the image is eaten by noise,\n",
    "                                          digits=range(10), # only include this digits\n",
    "                                          test = False\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5, 28, 28)\n",
      "(60000, 5)\n",
      "(10000, 5, 28, 28) (10000, 5, 28, 28)\n",
      "(10000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(hmnist.train_images.shape,hmnist.train_targets.shape)\n",
    "print(hmnist.train_rotations.shape)\n",
    "print(hmnist.test_images.shape,hmnist.test_targets.shape)\n",
    "print(hmnist.test_rotations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtsAAACaCAYAAAB464RIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2QYAAACcMNkGAAAAnPw/a9fY9lq7asMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x576 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "case = 4\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "for i, image in enumerate(hmnist.test_images[case]):\n",
    "    fig.add_subplot(1, 6, i+1)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtsAAACaCAYAAAB464RIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZxOFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x576 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "for i, image in enumerate(hmnist.test_targets[case]):\n",
    "    fig.add_subplot(1, 6, i+1)\n",
    "    plt.imshow(image,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.42973116 -51.04980853  88.38982625 -44.562548   -22.72210947]\n"
     ]
    }
   ],
   "source": [
    "print(hmnist.test_rotations[case])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the data shape\n",
    "this part should be different for different model, the q-RNN model does not igorned the sequencial dependency within the dataset, so we don't need to flat the dataset. So that the dataset should be that given a sequence of noisy image $\\{p_i\\}$ and a sequences of action $\\{u_i\\}$, the target should be the image of next timestep given action $u_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5) (60000, 5, 28, 28)\n",
      "(10000, 5, 28, 28) (10000, 5) (10000, 5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "train_X = hmnist.train_images\n",
    "train_u = hmnist.train_rotations\n",
    "train_Y = hmnist.train_targets\n",
    "test_X = hmnist.test_images\n",
    "test_u = hmnist.test_rotations\n",
    "test_Y = hmnist.test_targets\n",
    "print(train_X.shape,train_u.shape,train_Y.shape)\n",
    "print(test_X.shape,test_u.shape,test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5) (60000, 5, 28, 28)\n",
      "(10000, 5, 28, 28) (10000, 5) (10000, 5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#no adjustment needed\n",
    "\n",
    "#train_Y = train_Y[:,4,:,:]\n",
    "#test_Y = test_Y[:,4,:,:]\n",
    "\n",
    "print(train_X.shape,train_u.shape,train_Y.shape)\n",
    "print(test_X.shape,test_u.shape,test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ntrain_loader = torch.utils.data.DataLoader(\\n    datasets.MNIST('./data', train=True, download=True,\\n                   transform=transforms.ToTensor()),\\n    batch_size=batch_size, shuffle=True, **kwargs)\\n\\n\\n\\ntest_loader = torch.utils.data.DataLoader(\\n    datasets.MNIST('./data', train=False, transform=transforms.ToTensor()),\\n    batch_size=batch_size, shuffle=True, **kwargs)\\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HMNISTDataSet():\n",
    "    def __init__(self, train_img, train_act, train_tar, test_img, test_act, test_tar, test = False, transform=None):\n",
    "        self.test = test\n",
    "        self.transform = transform\n",
    "\n",
    "        if (self.test == False):\n",
    "          self.images = train_img\n",
    "          self.targets = train_tar\n",
    "          self.rotations = train_act\n",
    "\n",
    "        else:      \n",
    "          self.images = test_img\n",
    "          self.targets = test_tar\n",
    "          self.rotations = test_act\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform is not None:\n",
    "            img = torch.zeros((len(self.images[index]),1,28,28))\n",
    "            for i in range(len(self.images[index])):\n",
    "                img[i] = self.transform(self.images[index][i].reshape(28,28,1))\n",
    "            tar = torch.zeros((len(self.targets[index]),1,28,28))\n",
    "            for i in range(len(self.targets[index])):\n",
    "                tar[i] = self.transform(self.targets[index][i].reshape(28,28,1))\n",
    "                \n",
    "            rot = torch.tensor(self.rotations[index])\n",
    "        return img, rot, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HMNISTDataSet(train_X, train_u, train_Y, test_X, test_u, test_Y, test = False, transform = transforms.ToTensor())\n",
    "test_set = HMNISTDataSet(train_X, train_u, train_Y, test_X, test_u, test_Y, test = True, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c = train_set.__getitem__(3)\n",
    "c.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, h_d)\n",
    "        self.fc2 = nn.Linear(h_d,128)\n",
    "        self.fc21 = nn.Linear(128, l_d)\n",
    "        self.fc22 = nn.Linear(128, l_d)\n",
    "        \n",
    "        #transition layer\n",
    "        input_dim = l_d + u_d\n",
    "        self.rnn_mu = nn.RNN(input_size=input_dim,hidden_size=l_d,batch_first=True)\n",
    "        self.rnn_sigma = nn.RNN(input_size=input_dim,hidden_size=l_d,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(l_d, 128)\n",
    "        self.fc4 = nn.Linear(128,h_d)\n",
    "        self.fc5 = nn.Linear(h_d, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc21(h2), self.fc22(h2)\n",
    "\n",
    "    def reparameterize1(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def transition(self, z, u):\n",
    "        rnn_input = torch.cat((z,u),dim=2)\n",
    "        mu2,_ = self.rnn_mu(rnn_input)\n",
    "        logvar2,_ = self.rnn_sigma(rnn_input)\n",
    "        return mu2,logvar2\n",
    "    \n",
    "    def reparameterize2(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        h4 = F.relu(self.fc4(h3))\n",
    "        return torch.sigmoid(self.fc5(h4))\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        mu1, logvar1 = self.encode(x.view(-1, 784))\n",
    "        z1 = self.reparameterize1(mu1, logvar1)\n",
    "        z1 = z1.reshape(-1,5,32)\n",
    "        u = u.float()\n",
    "        mu2, logvar2 = self.transition(z1,u.reshape(-1,5,1))\n",
    "        z2 = self.reparameterize2(mu2, logvar2)\n",
    "        \n",
    "        return self.decode(z2), mu2, logvar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "#adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (img, action, target) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        action = action.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(img,action)\n",
    "        loss = loss_function(recon_batch, target, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        #if batch_idx % log_interval == 0:\n",
    "         #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          #      epoch, batch_idx * len(img), len(train_loader.dataset),\n",
    "           #     100. * batch_idx / len(train_loader),\n",
    "            #    loss.item() / len(img)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (img, action, target) in enumerate(test_loader):\n",
    "            img = img.to(device)\n",
    "            action = action.to(device)\n",
    "            target = target.to(device)\n",
    "            recon_batch, mu, logvar = model(img, action)\n",
    "            test_loss += loss_function(recon_batch, target, mu, logvar).item()\n",
    "            if(epoch > epochs - 10):\n",
    "                if i == 0:\n",
    "                    n = np.random.randint(0,batch_size)\n",
    "                    comparison = torch.cat([target[n],\n",
    "                                          recon_batch.view(batch_size, 5, 1, 28, 28)[n]],dim=0)\n",
    "                    save_image(comparison.cpu(),\n",
    "                             'results_rnn_old_test/reconstruction_' + str(epoch) + '.png', nrow=5)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 1104.1885\n",
      "====> Test set loss: 1028.5254\n",
      "====> Epoch: 2 Average loss: 1021.2823\n",
      "====> Test set loss: 1012.1246\n",
      "====> Epoch: 3 Average loss: 985.0143\n",
      "====> Test set loss: 950.1792\n",
      "====> Epoch: 4 Average loss: 931.6177\n",
      "====> Test set loss: 912.2539\n",
      "====> Epoch: 5 Average loss: 902.8930\n",
      "====> Test set loss: 888.3585\n",
      "====> Epoch: 6 Average loss: 883.6829\n",
      "====> Test set loss: 872.7368\n",
      "====> Epoch: 7 Average loss: 869.5415\n",
      "====> Test set loss: 860.1476\n",
      "====> Epoch: 8 Average loss: 858.0272\n",
      "====> Test set loss: 850.8579\n",
      "====> Epoch: 9 Average loss: 849.7733\n",
      "====> Test set loss: 845.9712\n",
      "====> Epoch: 10 Average loss: 842.6983\n",
      "====> Test set loss: 839.8256\n",
      "====> Epoch: 11 Average loss: 837.6835\n",
      "====> Test set loss: 834.7268\n",
      "====> Epoch: 12 Average loss: 833.3835\n",
      "====> Test set loss: 830.6245\n",
      "====> Epoch: 13 Average loss: 829.4545\n",
      "====> Test set loss: 829.9037\n",
      "====> Epoch: 14 Average loss: 825.7699\n",
      "====> Test set loss: 825.3977\n",
      "====> Epoch: 15 Average loss: 823.2738\n",
      "====> Test set loss: 824.4147\n",
      "====> Epoch: 16 Average loss: 820.2934\n",
      "====> Test set loss: 820.7212\n",
      "====> Epoch: 17 Average loss: 817.8402\n",
      "====> Test set loss: 824.2249\n",
      "====> Epoch: 18 Average loss: 816.2462\n",
      "====> Test set loss: 815.4750\n",
      "====> Epoch: 19 Average loss: 813.6788\n",
      "====> Test set loss: 814.9669\n",
      "====> Epoch: 20 Average loss: 812.3570\n",
      "====> Test set loss: 813.4657\n",
      "====> Epoch: 21 Average loss: 810.1518\n",
      "====> Test set loss: 812.3762\n",
      "====> Epoch: 22 Average loss: 808.6927\n",
      "====> Test set loss: 811.6133\n",
      "====> Epoch: 23 Average loss: 806.9215\n",
      "====> Test set loss: 810.2841\n",
      "====> Epoch: 24 Average loss: 805.7723\n",
      "====> Test set loss: 810.2033\n",
      "====> Epoch: 25 Average loss: 804.5152\n",
      "====> Test set loss: 808.6350\n",
      "====> Epoch: 26 Average loss: 803.0132\n",
      "====> Test set loss: 806.5067\n",
      "====> Epoch: 27 Average loss: 801.4027\n",
      "====> Test set loss: 805.9970\n",
      "====> Epoch: 28 Average loss: 800.8757\n",
      "====> Test set loss: 806.0251\n",
      "====> Epoch: 29 Average loss: 799.0199\n",
      "====> Test set loss: 804.8713\n",
      "====> Epoch: 30 Average loss: 798.0568\n",
      "====> Test set loss: 802.2625\n",
      "====> Epoch: 31 Average loss: 796.8841\n",
      "====> Test set loss: 809.0026\n",
      "====> Epoch: 32 Average loss: 796.2655\n",
      "====> Test set loss: 803.3775\n",
      "====> Epoch: 33 Average loss: 795.3885\n",
      "====> Test set loss: 800.4510\n",
      "====> Epoch: 34 Average loss: 794.6611\n",
      "====> Test set loss: 800.6478\n",
      "====> Epoch: 35 Average loss: 793.6128\n",
      "====> Test set loss: 799.0220\n",
      "====> Epoch: 36 Average loss: 792.8310\n",
      "====> Test set loss: 799.6153\n",
      "====> Epoch: 37 Average loss: 792.7245\n",
      "====> Test set loss: 798.9220\n",
      "====> Epoch: 38 Average loss: 791.3229\n",
      "====> Test set loss: 799.4282\n",
      "====> Epoch: 39 Average loss: 790.9512\n",
      "====> Test set loss: 798.8116\n",
      "====> Epoch: 40 Average loss: 790.1116\n",
      "====> Test set loss: 798.6768\n",
      "====> Epoch: 41 Average loss: 789.1317\n",
      "====> Test set loss: 796.2938\n",
      "====> Epoch: 42 Average loss: 788.8732\n",
      "====> Test set loss: 795.2237\n",
      "====> Epoch: 43 Average loss: 787.8400\n",
      "====> Test set loss: 797.9460\n",
      "====> Epoch: 44 Average loss: 787.5516\n",
      "====> Test set loss: 795.7984\n",
      "====> Epoch: 45 Average loss: 786.9874\n",
      "====> Test set loss: 795.0875\n",
      "====> Epoch: 46 Average loss: 786.3932\n",
      "====> Test set loss: 798.4302\n",
      "====> Epoch: 47 Average loss: 786.5282\n",
      "====> Test set loss: 798.4094\n",
      "====> Epoch: 48 Average loss: 785.7149\n",
      "====> Test set loss: 794.3746\n",
      "====> Epoch: 49 Average loss: 785.3447\n",
      "====> Test set loss: 795.8230\n",
      "====> Epoch: 50 Average loss: 784.5086\n",
      "====> Test set loss: 796.6604\n",
      "====> Epoch: 51 Average loss: 784.0471\n",
      "====> Test set loss: 795.4600\n",
      "====> Epoch: 52 Average loss: 783.2770\n",
      "====> Test set loss: 796.9709\n",
      "====> Epoch: 53 Average loss: 783.2187\n",
      "====> Test set loss: 793.1835\n",
      "====> Epoch: 54 Average loss: 782.3874\n",
      "====> Test set loss: 792.3921\n",
      "====> Epoch: 55 Average loss: 782.3822\n",
      "====> Test set loss: 794.1296\n",
      "====> Epoch: 56 Average loss: 781.7710\n",
      "====> Test set loss: 796.6809\n",
      "====> Epoch: 57 Average loss: 781.6145\n",
      "====> Test set loss: 792.5815\n",
      "====> Epoch: 58 Average loss: 780.9437\n",
      "====> Test set loss: 794.3852\n",
      "====> Epoch: 59 Average loss: 780.5938\n",
      "====> Test set loss: 792.7072\n",
      "====> Epoch: 60 Average loss: 779.8324\n",
      "====> Test set loss: 791.3673\n",
      "====> Epoch: 61 Average loss: 779.4998\n",
      "====> Test set loss: 792.7551\n",
      "====> Epoch: 62 Average loss: 779.5949\n",
      "====> Test set loss: 792.1046\n",
      "====> Epoch: 63 Average loss: 778.8068\n",
      "====> Test set loss: 791.7383\n",
      "====> Epoch: 64 Average loss: 778.5985\n",
      "====> Test set loss: 790.1506\n",
      "====> Epoch: 65 Average loss: 778.4843\n",
      "====> Test set loss: 791.7357\n",
      "====> Epoch: 66 Average loss: 777.7311\n",
      "====> Test set loss: 793.5517\n",
      "====> Epoch: 67 Average loss: 777.4930\n",
      "====> Test set loss: 791.8177\n",
      "====> Epoch: 68 Average loss: 777.1238\n",
      "====> Test set loss: 790.2693\n",
      "====> Epoch: 69 Average loss: 776.4814\n",
      "====> Test set loss: 790.5852\n",
      "====> Epoch: 70 Average loss: 776.5091\n",
      "====> Test set loss: 789.6438\n",
      "====> Epoch: 71 Average loss: 776.5629\n",
      "====> Test set loss: 789.8628\n",
      "====> Epoch: 72 Average loss: 775.9519\n",
      "====> Test set loss: 789.5348\n",
      "====> Epoch: 73 Average loss: 775.4856\n",
      "====> Test set loss: 790.8083\n",
      "====> Epoch: 74 Average loss: 775.2380\n",
      "====> Test set loss: 789.0577\n",
      "====> Epoch: 75 Average loss: 774.7272\n",
      "====> Test set loss: 789.2937\n",
      "====> Epoch: 76 Average loss: 774.6037\n",
      "====> Test set loss: 788.9920\n",
      "====> Epoch: 77 Average loss: 774.6966\n",
      "====> Test set loss: 788.8309\n",
      "====> Epoch: 78 Average loss: 773.9765\n",
      "====> Test set loss: 788.4928\n",
      "====> Epoch: 79 Average loss: 774.0202\n",
      "====> Test set loss: 789.9050\n",
      "====> Epoch: 80 Average loss: 773.5218\n",
      "====> Test set loss: 789.5497\n",
      "====> Epoch: 81 Average loss: 773.1499\n",
      "====> Test set loss: 790.0188\n",
      "====> Epoch: 82 Average loss: 773.7161\n",
      "====> Test set loss: 788.8467\n",
      "====> Epoch: 83 Average loss: 773.1084\n",
      "====> Test set loss: 788.4244\n",
      "====> Epoch: 84 Average loss: 772.8393\n",
      "====> Test set loss: 788.9361\n",
      "====> Epoch: 85 Average loss: 772.3958\n",
      "====> Test set loss: 787.9731\n",
      "====> Epoch: 86 Average loss: 771.9362\n",
      "====> Test set loss: 788.8005\n",
      "====> Epoch: 87 Average loss: 772.0520\n",
      "====> Test set loss: 788.9817\n",
      "====> Epoch: 88 Average loss: 771.8009\n",
      "====> Test set loss: 788.3207\n",
      "====> Epoch: 89 Average loss: 771.0989\n",
      "====> Test set loss: 787.9522\n",
      "====> Epoch: 90 Average loss: 771.7695\n",
      "====> Test set loss: 788.1782\n",
      "====> Epoch: 91 Average loss: 771.1388\n",
      "====> Test set loss: 788.7647\n",
      "====> Epoch: 92 Average loss: 771.2564\n",
      "====> Test set loss: 788.0064\n",
      "====> Epoch: 93 Average loss: 770.1390\n",
      "====> Test set loss: 788.4685\n",
      "====> Epoch: 94 Average loss: 770.3865\n",
      "====> Test set loss: 789.2668\n",
      "====> Epoch: 95 Average loss: 770.0664\n",
      "====> Test set loss: 788.4725\n",
      "====> Epoch: 96 Average loss: 770.1293\n",
      "====> Test set loss: 789.8818\n",
      "====> Epoch: 97 Average loss: 770.0604\n",
      "====> Test set loss: 788.4494\n",
      "====> Epoch: 98 Average loss: 769.7966\n",
      "====> Test set loss: 788.7680\n",
      "====> Epoch: 99 Average loss: 769.8210\n",
      "====> Test set loss: 789.0159\n",
      "====> Epoch: 100 Average loss: 769.9113\n",
      "====> Test set loss: 787.4597\n",
      "====> Epoch: 101 Average loss: 769.4388\n",
      "====> Test set loss: 788.7163\n",
      "====> Epoch: 102 Average loss: 769.4744\n",
      "====> Test set loss: 789.3045\n",
      "====> Epoch: 103 Average loss: 768.8568\n",
      "====> Test set loss: 787.8442\n",
      "====> Epoch: 104 Average loss: 768.5160\n",
      "====> Test set loss: 788.9632\n",
      "====> Epoch: 105 Average loss: 768.5917\n",
      "====> Test set loss: 788.4752\n",
      "====> Epoch: 106 Average loss: 768.8521\n",
      "====> Test set loss: 787.3964\n",
      "====> Epoch: 107 Average loss: 767.9061\n",
      "====> Test set loss: 787.9448\n",
      "====> Epoch: 108 Average loss: 767.8784\n",
      "====> Test set loss: 787.5077\n",
      "====> Epoch: 109 Average loss: 767.8240\n",
      "====> Test set loss: 788.0295\n",
      "====> Epoch: 110 Average loss: 767.7854\n",
      "====> Test set loss: 787.3406\n",
      "====> Epoch: 111 Average loss: 767.4711\n",
      "====> Test set loss: 786.9106\n",
      "====> Epoch: 112 Average loss: 767.3890\n",
      "====> Test set loss: 787.0116\n",
      "====> Epoch: 113 Average loss: 767.3310\n",
      "====> Test set loss: 786.9828\n",
      "====> Epoch: 114 Average loss: 767.1686\n",
      "====> Test set loss: 786.9354\n",
      "====> Epoch: 115 Average loss: 766.7250\n",
      "====> Test set loss: 787.7300\n",
      "====> Epoch: 116 Average loss: 766.8404\n",
      "====> Test set loss: 787.6171\n",
      "====> Epoch: 117 Average loss: 767.0275\n",
      "====> Test set loss: 787.1935\n",
      "====> Epoch: 118 Average loss: 767.1103\n",
      "====> Test set loss: 788.5168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 119 Average loss: 766.5901\n",
      "====> Test set loss: 787.3593\n",
      "====> Epoch: 120 Average loss: 766.1763\n",
      "====> Test set loss: 787.3212\n",
      "====> Epoch: 121 Average loss: 766.2079\n",
      "====> Test set loss: 787.8027\n",
      "====> Epoch: 122 Average loss: 766.3897\n",
      "====> Test set loss: 787.0106\n",
      "====> Epoch: 123 Average loss: 766.4188\n",
      "====> Test set loss: 788.3455\n",
      "====> Epoch: 124 Average loss: 766.0563\n",
      "====> Test set loss: 788.4233\n",
      "====> Epoch: 125 Average loss: 766.1097\n",
      "====> Test set loss: 786.1219\n",
      "====> Epoch: 126 Average loss: 765.1936\n",
      "====> Test set loss: 787.8603\n",
      "====> Epoch: 127 Average loss: 765.3518\n",
      "====> Test set loss: 786.4095\n",
      "====> Epoch: 128 Average loss: 765.5217\n",
      "====> Test set loss: 788.3692\n",
      "====> Epoch: 129 Average loss: 765.6059\n",
      "====> Test set loss: 790.3953\n",
      "====> Epoch: 130 Average loss: 764.9229\n",
      "====> Test set loss: 787.2030\n",
      "====> Epoch: 131 Average loss: 765.2562\n",
      "====> Test set loss: 788.0334\n",
      "====> Epoch: 132 Average loss: 765.2870\n",
      "====> Test set loss: 787.1225\n",
      "====> Epoch: 133 Average loss: 765.0387\n",
      "====> Test set loss: 786.4699\n",
      "====> Epoch: 134 Average loss: 764.8736\n",
      "====> Test set loss: 790.7688\n",
      "====> Epoch: 135 Average loss: 764.4757\n",
      "====> Test set loss: 787.2094\n",
      "====> Epoch: 136 Average loss: 765.2129\n",
      "====> Test set loss: 787.7835\n",
      "====> Epoch: 137 Average loss: 764.4182\n",
      "====> Test set loss: 788.1798\n",
      "====> Epoch: 138 Average loss: 764.5667\n",
      "====> Test set loss: 787.9682\n",
      "====> Epoch: 139 Average loss: 763.9385\n",
      "====> Test set loss: 787.0245\n",
      "====> Epoch: 140 Average loss: 764.4304\n",
      "====> Test set loss: 788.5875\n",
      "====> Epoch: 141 Average loss: 764.1695\n",
      "====> Test set loss: 787.0752\n",
      "====> Epoch: 142 Average loss: 763.6409\n",
      "====> Test set loss: 787.0499\n",
      "====> Epoch: 143 Average loss: 763.5988\n",
      "====> Test set loss: 789.7823\n",
      "====> Epoch: 144 Average loss: 763.9840\n",
      "====> Test set loss: 787.6193\n",
      "====> Epoch: 145 Average loss: 763.3060\n",
      "====> Test set loss: 788.6260\n",
      "====> Epoch: 146 Average loss: 763.2857\n",
      "====> Test set loss: 786.2682\n",
      "====> Epoch: 147 Average loss: 763.5276\n",
      "====> Test set loss: 788.7623\n",
      "====> Epoch: 148 Average loss: 763.5756\n",
      "====> Test set loss: 786.2627\n",
      "====> Epoch: 149 Average loss: 763.0820\n",
      "====> Test set loss: 790.8929\n",
      "====> Epoch: 150 Average loss: 763.1760\n",
      "====> Test set loss: 788.5521\n",
      "====> Epoch: 151 Average loss: 763.2186\n",
      "====> Test set loss: 788.0224\n",
      "====> Epoch: 152 Average loss: 763.2153\n",
      "====> Test set loss: 788.1043\n",
      "====> Epoch: 153 Average loss: 763.2491\n",
      "====> Test set loss: 789.1902\n",
      "====> Epoch: 154 Average loss: 762.7369\n",
      "====> Test set loss: 787.3404\n",
      "====> Epoch: 155 Average loss: 762.5596\n",
      "====> Test set loss: 788.8860\n",
      "====> Epoch: 156 Average loss: 762.5580\n",
      "====> Test set loss: 787.4916\n",
      "====> Epoch: 157 Average loss: 762.5792\n",
      "====> Test set loss: 790.6808\n",
      "====> Epoch: 158 Average loss: 762.1524\n",
      "====> Test set loss: 787.0477\n",
      "====> Epoch: 159 Average loss: 762.4840\n",
      "====> Test set loss: 787.8839\n",
      "====> Epoch: 160 Average loss: 762.6783\n",
      "====> Test set loss: 787.0559\n",
      "====> Epoch: 161 Average loss: 762.7338\n",
      "====> Test set loss: 786.8508\n",
      "====> Epoch: 162 Average loss: 762.4005\n",
      "====> Test set loss: 789.2056\n",
      "====> Epoch: 163 Average loss: 762.0211\n",
      "====> Test set loss: 789.1512\n",
      "====> Epoch: 164 Average loss: 762.6368\n",
      "====> Test set loss: 788.3593\n",
      "====> Epoch: 165 Average loss: 761.4822\n",
      "====> Test set loss: 788.6807\n",
      "====> Epoch: 166 Average loss: 761.5089\n",
      "====> Test set loss: 787.7996\n",
      "====> Epoch: 167 Average loss: 761.8306\n",
      "====> Test set loss: 788.8042\n",
      "====> Epoch: 168 Average loss: 761.1777\n",
      "====> Test set loss: 788.1390\n",
      "====> Epoch: 169 Average loss: 761.6455\n",
      "====> Test set loss: 788.5294\n",
      "====> Epoch: 170 Average loss: 761.4360\n",
      "====> Test set loss: 788.8961\n",
      "====> Epoch: 171 Average loss: 761.1419\n",
      "====> Test set loss: 787.5850\n",
      "====> Epoch: 172 Average loss: 761.5367\n",
      "====> Test set loss: 788.1741\n",
      "====> Epoch: 173 Average loss: 761.8061\n",
      "====> Test set loss: 789.3751\n",
      "====> Epoch: 174 Average loss: 761.0338\n",
      "====> Test set loss: 788.0915\n",
      "====> Epoch: 175 Average loss: 760.5812\n",
      "====> Test set loss: 787.6384\n",
      "====> Epoch: 176 Average loss: 760.6051\n",
      "====> Test set loss: 788.5961\n",
      "====> Epoch: 177 Average loss: 761.0883\n",
      "====> Test set loss: 788.3094\n",
      "====> Epoch: 178 Average loss: 760.7174\n",
      "====> Test set loss: 789.8100\n",
      "====> Epoch: 179 Average loss: 761.0341\n",
      "====> Test set loss: 788.5618\n",
      "====> Epoch: 180 Average loss: 760.8429\n",
      "====> Test set loss: 786.5913\n",
      "====> Epoch: 181 Average loss: 760.7102\n",
      "====> Test set loss: 789.6083\n",
      "====> Epoch: 182 Average loss: 760.8139\n",
      "====> Test set loss: 788.2833\n",
      "====> Epoch: 183 Average loss: 760.1741\n",
      "====> Test set loss: 789.2183\n",
      "====> Epoch: 184 Average loss: 760.6551\n",
      "====> Test set loss: 790.7825\n",
      "====> Epoch: 185 Average loss: 760.7260\n",
      "====> Test set loss: 788.6192\n",
      "====> Epoch: 186 Average loss: 760.8859\n",
      "====> Test set loss: 789.9160\n",
      "====> Epoch: 187 Average loss: 759.8287\n",
      "====> Test set loss: 789.0333\n",
      "====> Epoch: 188 Average loss: 760.4745\n",
      "====> Test set loss: 788.0783\n",
      "====> Epoch: 189 Average loss: 759.5992\n",
      "====> Test set loss: 790.3047\n",
      "====> Epoch: 190 Average loss: 760.6005\n",
      "====> Test set loss: 788.6865\n",
      "====> Epoch: 191 Average loss: 760.0647\n",
      "====> Test set loss: 787.2943\n",
      "====> Epoch: 192 Average loss: 760.7268\n",
      "====> Test set loss: 789.0756\n",
      "====> Epoch: 193 Average loss: 759.4906\n",
      "====> Test set loss: 787.2045\n",
      "====> Epoch: 194 Average loss: 759.5491\n",
      "====> Test set loss: 789.8192\n",
      "====> Epoch: 195 Average loss: 760.0154\n",
      "====> Test set loss: 787.8325\n",
      "====> Epoch: 196 Average loss: 759.2602\n",
      "====> Test set loss: 788.6923\n",
      "====> Epoch: 197 Average loss: 759.2069\n",
      "====> Test set loss: 789.2769\n",
      "====> Epoch: 198 Average loss: 759.4555\n",
      "====> Test set loss: 789.3667\n",
      "====> Epoch: 199 Average loss: 759.2574\n",
      "====> Test set loss: 788.9308\n",
      "====> Epoch: 200 Average loss: 759.4572\n",
      "====> Test set loss: 789.4312\n"
     ]
    }
   ],
   "source": [
    "import warnings;\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    with torch.no_grad():\n",
    "        if(epoch > epochs-10):\n",
    "            sample = torch.randn(64, l_d).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results_rnn_old_test/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkQxSuIuIZJDCXUQkg/4/xIgnQMFKWusAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(train_losses),'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wcoz59unf9mygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(test_losses),'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
