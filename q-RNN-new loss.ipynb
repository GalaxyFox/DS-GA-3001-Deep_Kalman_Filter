{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ba715104d0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\"\"\"\n",
    "batch_size = 128\n",
    "seed = 1\n",
    "epochs = 300\n",
    "cuda = True\n",
    "log_interval = 10\n",
    "sample_size = 10\n",
    "h_d = 512\n",
    "l_d = 32\n",
    "u_d = 1\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hmnist dataset\n",
    "import healing_mnist_indep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmnist = healing_mnist_indep.HealingMNIST(seq_len=5, # 5 rotations of each digit\n",
    "                                          square_count=0, # 3 out of 5 images have a square added to them\n",
    "                                          square_size=5, # the square is 5x5\n",
    "                                          noise_ratio=0.1, # on average, 20% of the image is eaten by noise,\n",
    "                                          digits=range(10), # only include this digits\n",
    "                                          test = False\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5, 28, 28)\n",
      "(60000, 5)\n",
      "(10000, 5, 28, 28) (10000, 5, 28, 28)\n",
      "(10000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(hmnist.train_images.shape,hmnist.train_targets.shape)\n",
    "print(hmnist.train_rotations.shape)\n",
    "print(hmnist.test_images.shape,hmnist.test_targets.shape)\n",
    "print(hmnist.test_rotations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtsAAACaCAYAAAB464RIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2i7wAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x576 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "case = 4\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "for i, image in enumerate(hmnist.test_images[case]):\n",
    "    fig.add_subplot(1, 6, i+1)\n",
    "    plt.imshow(image,cmap='gray')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtsAAACaCAYAAAB464RIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIBONNsAANDJ/wPKXDNDHkPkMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x576 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,8))\n",
    "for i, image in enumerate(hmnist.test_targets[case]):\n",
    "    fig.add_subplot(1, 6, i+1)\n",
    "    plt.imshow(image,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5.93870441   4.03446702  -2.34962547  21.43412511  25.80954221]\n"
     ]
    }
   ],
   "source": [
    "print(hmnist.test_rotations[case])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the data shape\n",
    "this part should be different for different model, the q-RNN model does not igorned the sequencial dependency within the dataset, so we don't need to flat the dataset. So that the dataset should be that given a sequence of noisy image $\\{p_i\\}$ and a sequences of action $\\{u_i\\}$, the target should be the image of next timestep given action $u_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5) (60000, 5, 28, 28)\n",
      "(10000, 5, 28, 28) (10000, 5) (10000, 5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "train_X = hmnist.train_images\n",
    "train_u = hmnist.train_rotations\n",
    "train_Y = hmnist.train_targets\n",
    "test_X = hmnist.test_images\n",
    "test_u = hmnist.test_rotations\n",
    "test_Y = hmnist.test_targets\n",
    "print(train_X.shape,train_u.shape,train_Y.shape)\n",
    "print(test_X.shape,test_u.shape,test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5, 28, 28) (60000, 5) (60000, 5, 28, 28)\n",
      "(10000, 5, 28, 28) (10000, 5) (10000, 5, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#no adjustment needed\n",
    "\n",
    "#train_Y = train_Y[:,4,:,:]\n",
    "#test_Y = test_Y[:,4,:,:]\n",
    "\n",
    "print(train_X.shape,train_u.shape,train_Y.shape)\n",
    "print(test_X.shape,test_u.shape,test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ntrain_loader = torch.utils.data.DataLoader(\\n    datasets.MNIST('./data', train=True, download=True,\\n                   transform=transforms.ToTensor()),\\n    batch_size=batch_size, shuffle=True, **kwargs)\\n\\n\\n\\ntest_loader = torch.utils.data.DataLoader(\\n    datasets.MNIST('./data', train=False, transform=transforms.ToTensor()),\\n    batch_size=batch_size, shuffle=True, **kwargs)\\n\\n\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HMNISTDataSet():\n",
    "    def __init__(self, train_img, train_act, train_tar, test_img, test_act, test_tar, test = False, transform=None):\n",
    "        self.test = test\n",
    "        self.transform = transform\n",
    "\n",
    "        if (self.test == False):\n",
    "          self.images = train_img\n",
    "          self.targets = train_tar\n",
    "          self.rotations = train_act\n",
    "\n",
    "        else:      \n",
    "          self.images = test_img\n",
    "          self.targets = test_tar\n",
    "          self.rotations = test_act\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.transform is not None:\n",
    "            img = torch.zeros((len(self.images[index]),1,28,28))\n",
    "            for i in range(len(self.images[index])):\n",
    "                img[i] = self.transform(self.images[index][i].reshape(28,28,1))\n",
    "            tar = torch.zeros((len(self.targets[index]),1,28,28))\n",
    "            for i in range(len(self.targets[index])):\n",
    "                tar[i] = self.transform(self.targets[index][i].reshape(28,28,1))\n",
    "                \n",
    "            rot = torch.tensor(self.rotations[index])\n",
    "        return img, rot, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HMNISTDataSet(train_X, train_u, train_Y, test_X, test_u, test_Y, test = False, transform = transforms.ToTensor())\n",
    "test_set = HMNISTDataSet(train_X, train_u, train_Y, test_X, test_u, test_Y, test = True, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 28, 28])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c = train_set.__getitem__(3)\n",
    "c.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, h_d)\n",
    "        self.fc2 = nn.Linear(h_d,128)\n",
    "        self.fc21 = nn.Linear(128, l_d)\n",
    "        self.fc22 = nn.Linear(128, l_d)\n",
    "        \n",
    "        #transition layer\n",
    "        input_dim = l_d + u_d\n",
    "        self.rnn_mu = nn.RNN(input_size=input_dim,hidden_size=l_d,batch_first=True)\n",
    "        self.rnn_sigma = nn.RNN(input_size=input_dim,hidden_size=l_d,batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.fc3 = nn.Linear(l_d, 128)\n",
    "        self.fc4 = nn.Linear(128,h_d)\n",
    "        self.fc5 = nn.Linear(h_d, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc21(h2), self.fc22(h2)\n",
    "\n",
    "    def reparameterize1(self, mu, logvar, n=1):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def transition(self, z, u):\n",
    "        rnn_input = torch.cat((z,u),dim=2)\n",
    "        mu2,_ = self.rnn_mu(rnn_input)\n",
    "        logvar2,_ = self.rnn_sigma(rnn_input)\n",
    "        return mu2,logvar2\n",
    "    \n",
    "    def reparameterize2(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        h4 = F.relu(self.fc4(h3))\n",
    "        return torch.sigmoid(self.fc5(h4))\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        mu1, logvar1 = self.encode(x.view(-1, 784))\n",
    "        mu = torch.empty(sample_size, x.shape[0], x.shape[1], l_d , dtype=torch.float).to(device)\n",
    "        logvar = torch.empty(sample_size, x.shape[0], x.shape[1], l_d , dtype=torch.float).to(device)\n",
    "        for i in range(sample_size):\n",
    "            z1 = self.reparameterize1(mu1, logvar1)\n",
    "            z1 = z1.reshape(-1,5,32)\n",
    "            u = u.float()\n",
    "            mu2, logvar2 = self.transition(z1,u.reshape(-1,5,1))\n",
    "            mu[i] = mu2\n",
    "            logvar[i] = logvar2\n",
    "        z2 = self.reparameterize2(mu[0], logvar[0])\n",
    "        \n",
    "        return self.decode(z2), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "#adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar[0,:,0,:] - mu[0,:,0,:].pow(2) - logvar[0,:,0,:].exp())\n",
    "    EKLD = 0\n",
    "    for i in range(1,5):        \n",
    "        tmp = -0.5 * torch.sum(1 + logvar[:,:,i,:] - mu[:,:,i,:].pow(2) - logvar[:,:,i,:].exp())/sample_size\n",
    "        EKLD += tmp\n",
    "    return BCE + KLD + EKLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training and testing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (img, action, target) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        action = action.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(img,action)\n",
    "        loss = loss_function(recon_batch, target, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        #if batch_idx % log_interval == 0:\n",
    "         #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          #      epoch, batch_idx * len(img), len(train_loader.dataset),\n",
    "           #     100. * batch_idx / len(train_loader),\n",
    "            #    loss.item() / len(img)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (img, action, target) in enumerate(test_loader):\n",
    "            img = img.to(device)\n",
    "            action = action.to(device)\n",
    "            target = target.to(device)\n",
    "            recon_batch, mu, logvar = model(img, action)\n",
    "            test_loss += loss_function(recon_batch, target, mu, logvar).item()\n",
    "            if(epoch > epochs - 10):\n",
    "                if i == 0:\n",
    "                    n = np.random.randint(0,batch_size)\n",
    "                    comparison = torch.cat([target[n],\n",
    "                                          recon_batch.view(batch_size, 5, 1, 28, 28)[n]],dim=0)\n",
    "                    save_image(comparison.cpu(),\n",
    "                             'results/reconstruction_' + str(epoch) + '.png', nrow=5)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 1103.6548\n",
      "====> Test set loss: 1026.9961\n",
      "====> Epoch: 2 Average loss: 1021.5959\n",
      "====> Test set loss: 1011.6180\n",
      "====> Epoch: 3 Average loss: 981.1678\n",
      "====> Test set loss: 944.6881\n",
      "====> Epoch: 4 Average loss: 926.1772\n",
      "====> Test set loss: 908.1030\n",
      "====> Epoch: 5 Average loss: 897.9745\n",
      "====> Test set loss: 886.3565\n",
      "====> Epoch: 6 Average loss: 879.4640\n",
      "====> Test set loss: 871.2391\n",
      "====> Epoch: 7 Average loss: 867.1103\n",
      "====> Test set loss: 860.1757\n",
      "====> Epoch: 8 Average loss: 857.4125\n",
      "====> Test set loss: 853.7268\n",
      "====> Epoch: 9 Average loss: 849.0622\n",
      "====> Test set loss: 845.5340\n",
      "====> Epoch: 10 Average loss: 842.0342\n",
      "====> Test set loss: 841.2816\n",
      "====> Epoch: 11 Average loss: 836.3677\n",
      "====> Test set loss: 834.6133\n",
      "====> Epoch: 12 Average loss: 831.3019\n",
      "====> Test set loss: 831.2250\n",
      "====> Epoch: 13 Average loss: 827.6014\n",
      "====> Test set loss: 828.7788\n",
      "====> Epoch: 14 Average loss: 824.0785\n",
      "====> Test set loss: 823.9885\n",
      "====> Epoch: 15 Average loss: 821.0787\n",
      "====> Test set loss: 823.0097\n",
      "====> Epoch: 16 Average loss: 818.6067\n",
      "====> Test set loss: 820.3551\n",
      "====> Epoch: 17 Average loss: 816.7809\n",
      "====> Test set loss: 818.3423\n",
      "====> Epoch: 18 Average loss: 814.8359\n",
      "====> Test set loss: 818.1488\n",
      "====> Epoch: 19 Average loss: 812.7546\n",
      "====> Test set loss: 816.1214\n",
      "====> Epoch: 20 Average loss: 810.9734\n",
      "====> Test set loss: 816.3890\n",
      "====> Epoch: 21 Average loss: 809.1370\n",
      "====> Test set loss: 813.7267\n",
      "====> Epoch: 22 Average loss: 807.6006\n",
      "====> Test set loss: 814.0164\n",
      "====> Epoch: 23 Average loss: 806.3115\n",
      "====> Test set loss: 811.5565\n",
      "====> Epoch: 24 Average loss: 804.3895\n",
      "====> Test set loss: 811.2594\n",
      "====> Epoch: 25 Average loss: 803.5887\n",
      "====> Test set loss: 809.3697\n",
      "====> Epoch: 26 Average loss: 802.3138\n",
      "====> Test set loss: 807.1282\n",
      "====> Epoch: 27 Average loss: 801.4134\n",
      "====> Test set loss: 805.3399\n",
      "====> Epoch: 28 Average loss: 800.1635\n",
      "====> Test set loss: 807.1686\n",
      "====> Epoch: 29 Average loss: 798.9941\n",
      "====> Test set loss: 805.5311\n",
      "====> Epoch: 30 Average loss: 798.2627\n",
      "====> Test set loss: 803.7453\n",
      "====> Epoch: 31 Average loss: 797.1399\n",
      "====> Test set loss: 804.2385\n",
      "====> Epoch: 32 Average loss: 796.4302\n",
      "====> Test set loss: 804.7262\n",
      "====> Epoch: 33 Average loss: 795.3772\n",
      "====> Test set loss: 805.1472\n",
      "====> Epoch: 34 Average loss: 794.4616\n",
      "====> Test set loss: 803.9292\n",
      "====> Epoch: 35 Average loss: 793.4183\n",
      "====> Test set loss: 802.0468\n",
      "====> Epoch: 36 Average loss: 793.3625\n",
      "====> Test set loss: 801.6560\n",
      "====> Epoch: 37 Average loss: 792.1719\n",
      "====> Test set loss: 802.4262\n",
      "====> Epoch: 38 Average loss: 792.1442\n",
      "====> Test set loss: 804.3769\n",
      "====> Epoch: 39 Average loss: 791.2889\n",
      "====> Test set loss: 800.9468\n",
      "====> Epoch: 40 Average loss: 790.9898\n",
      "====> Test set loss: 800.2051\n",
      "====> Epoch: 41 Average loss: 789.8455\n",
      "====> Test set loss: 800.5349\n",
      "====> Epoch: 42 Average loss: 789.6299\n",
      "====> Test set loss: 799.7897\n",
      "====> Epoch: 43 Average loss: 789.0900\n",
      "====> Test set loss: 800.2518\n",
      "====> Epoch: 44 Average loss: 788.4790\n",
      "====> Test set loss: 797.8614\n",
      "====> Epoch: 45 Average loss: 787.6343\n",
      "====> Test set loss: 799.5928\n",
      "====> Epoch: 46 Average loss: 787.1835\n",
      "====> Test set loss: 799.5954\n",
      "====> Epoch: 47 Average loss: 786.9504\n",
      "====> Test set loss: 798.2383\n",
      "====> Epoch: 48 Average loss: 786.5400\n",
      "====> Test set loss: 798.0333\n",
      "====> Epoch: 49 Average loss: 785.7631\n",
      "====> Test set loss: 799.2373\n",
      "====> Epoch: 50 Average loss: 785.4191\n",
      "====> Test set loss: 797.8166\n",
      "====> Epoch: 51 Average loss: 785.0490\n",
      "====> Test set loss: 798.3234\n",
      "====> Epoch: 52 Average loss: 784.7169\n",
      "====> Test set loss: 798.1674\n",
      "====> Epoch: 53 Average loss: 784.0622\n",
      "====> Test set loss: 798.3052\n",
      "====> Epoch: 54 Average loss: 783.6761\n",
      "====> Test set loss: 796.0020\n",
      "====> Epoch: 55 Average loss: 783.2075\n",
      "====> Test set loss: 796.4494\n",
      "====> Epoch: 56 Average loss: 783.1896\n",
      "====> Test set loss: 796.0771\n",
      "====> Epoch: 57 Average loss: 782.9228\n",
      "====> Test set loss: 798.1185\n",
      "====> Epoch: 58 Average loss: 782.4379\n",
      "====> Test set loss: 797.2737\n",
      "====> Epoch: 59 Average loss: 782.1368\n",
      "====> Test set loss: 798.0755\n",
      "====> Epoch: 60 Average loss: 781.6356\n",
      "====> Test set loss: 796.1766\n",
      "====> Epoch: 61 Average loss: 780.8884\n",
      "====> Test set loss: 795.0939\n",
      "====> Epoch: 62 Average loss: 780.6851\n",
      "====> Test set loss: 797.3517\n",
      "====> Epoch: 63 Average loss: 780.7823\n",
      "====> Test set loss: 796.7681\n",
      "====> Epoch: 64 Average loss: 779.7081\n",
      "====> Test set loss: 795.3112\n",
      "====> Epoch: 65 Average loss: 779.4746\n",
      "====> Test set loss: 796.5222\n",
      "====> Epoch: 66 Average loss: 779.5197\n",
      "====> Test set loss: 796.2565\n",
      "====> Epoch: 67 Average loss: 778.8186\n",
      "====> Test set loss: 795.3510\n",
      "====> Epoch: 68 Average loss: 779.0179\n",
      "====> Test set loss: 795.8160\n",
      "====> Epoch: 69 Average loss: 778.5263\n",
      "====> Test set loss: 794.4272\n",
      "====> Epoch: 70 Average loss: 778.3188\n",
      "====> Test set loss: 793.7842\n",
      "====> Epoch: 71 Average loss: 778.2977\n",
      "====> Test set loss: 795.1182\n",
      "====> Epoch: 72 Average loss: 777.8960\n",
      "====> Test set loss: 794.7846\n",
      "====> Epoch: 73 Average loss: 777.7837\n",
      "====> Test set loss: 796.3127\n",
      "====> Epoch: 74 Average loss: 777.6870\n",
      "====> Test set loss: 793.9356\n",
      "====> Epoch: 75 Average loss: 777.2431\n",
      "====> Test set loss: 794.1353\n",
      "====> Epoch: 76 Average loss: 777.1755\n",
      "====> Test set loss: 793.3343\n",
      "====> Epoch: 77 Average loss: 776.6659\n",
      "====> Test set loss: 793.0469\n",
      "====> Epoch: 78 Average loss: 776.0576\n",
      "====> Test set loss: 794.5548\n",
      "====> Epoch: 79 Average loss: 776.2293\n",
      "====> Test set loss: 794.2918\n",
      "====> Epoch: 80 Average loss: 775.4300\n",
      "====> Test set loss: 794.4225\n",
      "====> Epoch: 81 Average loss: 775.7039\n",
      "====> Test set loss: 794.1445\n",
      "====> Epoch: 82 Average loss: 775.5729\n",
      "====> Test set loss: 794.2350\n",
      "====> Epoch: 83 Average loss: 775.0138\n",
      "====> Test set loss: 793.8831\n",
      "====> Epoch: 84 Average loss: 774.9179\n",
      "====> Test set loss: 793.0745\n",
      "====> Epoch: 85 Average loss: 774.6361\n",
      "====> Test set loss: 792.6968\n",
      "====> Epoch: 86 Average loss: 775.0679\n",
      "====> Test set loss: 793.0856\n",
      "====> Epoch: 87 Average loss: 774.4199\n",
      "====> Test set loss: 793.9877\n",
      "====> Epoch: 88 Average loss: 774.2960\n",
      "====> Test set loss: 792.3575\n",
      "====> Epoch: 89 Average loss: 773.6820\n",
      "====> Test set loss: 792.0089\n",
      "====> Epoch: 90 Average loss: 773.6461\n",
      "====> Test set loss: 793.3722\n",
      "====> Epoch: 91 Average loss: 773.5246\n",
      "====> Test set loss: 792.7925\n",
      "====> Epoch: 92 Average loss: 773.7633\n",
      "====> Test set loss: 792.6761\n",
      "====> Epoch: 93 Average loss: 773.0483\n",
      "====> Test set loss: 794.3941\n",
      "====> Epoch: 94 Average loss: 773.1719\n",
      "====> Test set loss: 791.9212\n",
      "====> Epoch: 95 Average loss: 772.7024\n",
      "====> Test set loss: 792.7109\n",
      "====> Epoch: 96 Average loss: 772.6656\n",
      "====> Test set loss: 793.1518\n",
      "====> Epoch: 97 Average loss: 772.4770\n",
      "====> Test set loss: 793.5837\n",
      "====> Epoch: 98 Average loss: 772.4360\n",
      "====> Test set loss: 792.4487\n",
      "====> Epoch: 99 Average loss: 772.7264\n",
      "====> Test set loss: 790.8978\n",
      "====> Epoch: 100 Average loss: 771.6840\n",
      "====> Test set loss: 791.1024\n",
      "====> Epoch: 101 Average loss: 771.6944\n",
      "====> Test set loss: 792.1328\n",
      "====> Epoch: 102 Average loss: 771.2630\n",
      "====> Test set loss: 793.1623\n",
      "====> Epoch: 103 Average loss: 771.3333\n",
      "====> Test set loss: 794.1135\n",
      "====> Epoch: 104 Average loss: 771.4859\n",
      "====> Test set loss: 791.6703\n",
      "====> Epoch: 105 Average loss: 771.0782\n",
      "====> Test set loss: 791.3171\n",
      "====> Epoch: 106 Average loss: 770.6088\n",
      "====> Test set loss: 791.6357\n",
      "====> Epoch: 107 Average loss: 770.7437\n",
      "====> Test set loss: 791.0167\n",
      "====> Epoch: 108 Average loss: 770.4430\n",
      "====> Test set loss: 792.6082\n",
      "====> Epoch: 109 Average loss: 770.3675\n",
      "====> Test set loss: 792.1110\n",
      "====> Epoch: 110 Average loss: 770.0147\n",
      "====> Test set loss: 790.3753\n",
      "====> Epoch: 111 Average loss: 769.9390\n",
      "====> Test set loss: 792.5626\n",
      "====> Epoch: 112 Average loss: 769.5124\n",
      "====> Test set loss: 791.8087\n",
      "====> Epoch: 113 Average loss: 769.7454\n",
      "====> Test set loss: 791.2677\n",
      "====> Epoch: 114 Average loss: 769.3932\n",
      "====> Test set loss: 790.3712\n",
      "====> Epoch: 115 Average loss: 769.1601\n",
      "====> Test set loss: 791.7537\n",
      "====> Epoch: 116 Average loss: 769.1239\n",
      "====> Test set loss: 790.9094\n",
      "====> Epoch: 117 Average loss: 768.8513\n",
      "====> Test set loss: 791.1009\n",
      "====> Epoch: 118 Average loss: 769.0594\n",
      "====> Test set loss: 792.5450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 119 Average loss: 768.6441\n",
      "====> Test set loss: 790.9211\n",
      "====> Epoch: 120 Average loss: 768.4867\n",
      "====> Test set loss: 789.8792\n",
      "====> Epoch: 121 Average loss: 768.1577\n",
      "====> Test set loss: 789.4425\n",
      "====> Epoch: 122 Average loss: 767.9120\n",
      "====> Test set loss: 789.6094\n",
      "====> Epoch: 123 Average loss: 768.0067\n",
      "====> Test set loss: 790.6927\n",
      "====> Epoch: 124 Average loss: 767.7738\n",
      "====> Test set loss: 791.9990\n",
      "====> Epoch: 125 Average loss: 768.0831\n",
      "====> Test set loss: 791.7088\n",
      "====> Epoch: 126 Average loss: 767.8597\n",
      "====> Test set loss: 792.3231\n",
      "====> Epoch: 127 Average loss: 767.7326\n",
      "====> Test set loss: 789.5397\n",
      "====> Epoch: 128 Average loss: 767.5408\n",
      "====> Test set loss: 791.9521\n",
      "====> Epoch: 129 Average loss: 767.2184\n",
      "====> Test set loss: 790.7419\n",
      "====> Epoch: 130 Average loss: 767.2962\n",
      "====> Test set loss: 790.5433\n",
      "====> Epoch: 131 Average loss: 767.0374\n",
      "====> Test set loss: 792.3177\n",
      "====> Epoch: 132 Average loss: 766.7622\n",
      "====> Test set loss: 788.7123\n",
      "====> Epoch: 133 Average loss: 767.0802\n",
      "====> Test set loss: 791.8067\n",
      "====> Epoch: 134 Average loss: 766.5449\n",
      "====> Test set loss: 791.0661\n",
      "====> Epoch: 135 Average loss: 766.7982\n",
      "====> Test set loss: 790.0533\n",
      "====> Epoch: 136 Average loss: 766.4710\n",
      "====> Test set loss: 791.1768\n",
      "====> Epoch: 137 Average loss: 766.9995\n",
      "====> Test set loss: 791.3284\n",
      "====> Epoch: 138 Average loss: 766.6530\n",
      "====> Test set loss: 788.5711\n",
      "====> Epoch: 139 Average loss: 766.6976\n",
      "====> Test set loss: 789.5979\n",
      "====> Epoch: 140 Average loss: 765.8769\n",
      "====> Test set loss: 791.0537\n",
      "====> Epoch: 141 Average loss: 765.6263\n",
      "====> Test set loss: 789.7772\n",
      "====> Epoch: 142 Average loss: 766.0538\n",
      "====> Test set loss: 792.0403\n",
      "====> Epoch: 143 Average loss: 765.7106\n",
      "====> Test set loss: 789.9282\n",
      "====> Epoch: 144 Average loss: 765.7403\n",
      "====> Test set loss: 790.4058\n",
      "====> Epoch: 145 Average loss: 765.6735\n",
      "====> Test set loss: 789.1452\n",
      "====> Epoch: 146 Average loss: 765.3596\n",
      "====> Test set loss: 789.8986\n",
      "====> Epoch: 147 Average loss: 765.2244\n",
      "====> Test set loss: 788.9849\n",
      "====> Epoch: 148 Average loss: 765.1027\n",
      "====> Test set loss: 789.1102\n",
      "====> Epoch: 149 Average loss: 764.6538\n",
      "====> Test set loss: 790.7851\n",
      "====> Epoch: 150 Average loss: 764.9199\n",
      "====> Test set loss: 789.4091\n",
      "====> Epoch: 151 Average loss: 764.7584\n",
      "====> Test set loss: 793.2217\n",
      "====> Epoch: 152 Average loss: 764.8762\n",
      "====> Test set loss: 789.5027\n",
      "====> Epoch: 153 Average loss: 764.9207\n",
      "====> Test set loss: 788.5729\n",
      "====> Epoch: 154 Average loss: 764.8956\n",
      "====> Test set loss: 790.0504\n",
      "====> Epoch: 155 Average loss: 764.4298\n",
      "====> Test set loss: 790.9979\n",
      "====> Epoch: 156 Average loss: 764.4574\n",
      "====> Test set loss: 789.3956\n",
      "====> Epoch: 157 Average loss: 764.0614\n",
      "====> Test set loss: 789.9143\n",
      "====> Epoch: 158 Average loss: 764.2462\n",
      "====> Test set loss: 788.3690\n",
      "====> Epoch: 159 Average loss: 764.1614\n",
      "====> Test set loss: 789.1504\n",
      "====> Epoch: 160 Average loss: 763.7226\n",
      "====> Test set loss: 789.9753\n",
      "====> Epoch: 161 Average loss: 764.3960\n",
      "====> Test set loss: 789.7839\n",
      "====> Epoch: 162 Average loss: 763.2622\n",
      "====> Test set loss: 789.9919\n",
      "====> Epoch: 163 Average loss: 763.5345\n",
      "====> Test set loss: 789.1316\n",
      "====> Epoch: 164 Average loss: 763.6043\n",
      "====> Test set loss: 790.1916\n",
      "====> Epoch: 165 Average loss: 763.3284\n",
      "====> Test set loss: 790.4484\n",
      "====> Epoch: 166 Average loss: 763.3069\n",
      "====> Test set loss: 791.2075\n",
      "====> Epoch: 167 Average loss: 763.6936\n",
      "====> Test set loss: 789.0677\n",
      "====> Epoch: 168 Average loss: 763.2055\n",
      "====> Test set loss: 790.4332\n",
      "====> Epoch: 169 Average loss: 762.9043\n",
      "====> Test set loss: 790.4819\n",
      "====> Epoch: 170 Average loss: 762.6901\n",
      "====> Test set loss: 790.9338\n",
      "====> Epoch: 171 Average loss: 763.0712\n",
      "====> Test set loss: 790.0597\n",
      "====> Epoch: 172 Average loss: 762.9903\n",
      "====> Test set loss: 789.5260\n",
      "====> Epoch: 173 Average loss: 763.3942\n",
      "====> Test set loss: 789.9175\n",
      "====> Epoch: 174 Average loss: 763.2501\n",
      "====> Test set loss: 788.7830\n",
      "====> Epoch: 175 Average loss: 762.5044\n",
      "====> Test set loss: 789.5418\n",
      "====> Epoch: 176 Average loss: 762.5760\n",
      "====> Test set loss: 789.5090\n",
      "====> Epoch: 177 Average loss: 762.4137\n",
      "====> Test set loss: 789.3137\n",
      "====> Epoch: 178 Average loss: 762.4105\n",
      "====> Test set loss: 791.3297\n",
      "====> Epoch: 179 Average loss: 762.2890\n",
      "====> Test set loss: 789.4751\n",
      "====> Epoch: 180 Average loss: 762.2920\n",
      "====> Test set loss: 790.3063\n",
      "====> Epoch: 181 Average loss: 762.3672\n",
      "====> Test set loss: 790.0207\n",
      "====> Epoch: 182 Average loss: 762.2135\n",
      "====> Test set loss: 789.1497\n",
      "====> Epoch: 183 Average loss: 761.9349\n",
      "====> Test set loss: 789.4973\n",
      "====> Epoch: 184 Average loss: 762.5871\n",
      "====> Test set loss: 787.9539\n",
      "====> Epoch: 185 Average loss: 762.0461\n",
      "====> Test set loss: 789.8347\n",
      "====> Epoch: 186 Average loss: 761.9498\n",
      "====> Test set loss: 789.7497\n",
      "====> Epoch: 187 Average loss: 761.7718\n",
      "====> Test set loss: 789.7664\n",
      "====> Epoch: 188 Average loss: 761.5854\n",
      "====> Test set loss: 789.7266\n",
      "====> Epoch: 189 Average loss: 761.5120\n",
      "====> Test set loss: 789.7016\n",
      "====> Epoch: 190 Average loss: 761.4173\n",
      "====> Test set loss: 789.0550\n",
      "====> Epoch: 191 Average loss: 761.6886\n",
      "====> Test set loss: 789.0081\n",
      "====> Epoch: 192 Average loss: 761.2675\n",
      "====> Test set loss: 790.0720\n",
      "====> Epoch: 193 Average loss: 761.7124\n",
      "====> Test set loss: 791.0625\n",
      "====> Epoch: 194 Average loss: 761.2788\n",
      "====> Test set loss: 788.9586\n",
      "====> Epoch: 195 Average loss: 761.0823\n",
      "====> Test set loss: 789.1959\n",
      "====> Epoch: 196 Average loss: 761.0904\n",
      "====> Test set loss: 789.8830\n",
      "====> Epoch: 197 Average loss: 760.6628\n",
      "====> Test set loss: 790.9563\n",
      "====> Epoch: 198 Average loss: 760.4549\n",
      "====> Test set loss: 789.4942\n",
      "====> Epoch: 199 Average loss: 760.6779\n",
      "====> Test set loss: 791.2841\n",
      "====> Epoch: 200 Average loss: 760.9658\n",
      "====> Test set loss: 789.8156\n",
      "====> Epoch: 201 Average loss: 760.6422\n",
      "====> Test set loss: 788.6517\n",
      "====> Epoch: 202 Average loss: 760.8090\n",
      "====> Test set loss: 789.4374\n",
      "====> Epoch: 203 Average loss: 760.7563\n",
      "====> Test set loss: 790.4334\n",
      "====> Epoch: 204 Average loss: 759.8009\n",
      "====> Test set loss: 789.9148\n",
      "====> Epoch: 205 Average loss: 760.0976\n",
      "====> Test set loss: 791.0821\n",
      "====> Epoch: 206 Average loss: 760.1118\n",
      "====> Test set loss: 790.4228\n",
      "====> Epoch: 207 Average loss: 760.2979\n",
      "====> Test set loss: 790.3183\n",
      "====> Epoch: 208 Average loss: 759.8029\n",
      "====> Test set loss: 789.2486\n",
      "====> Epoch: 209 Average loss: 759.7980\n",
      "====> Test set loss: 790.0711\n",
      "====> Epoch: 210 Average loss: 760.1419\n",
      "====> Test set loss: 790.0628\n",
      "====> Epoch: 211 Average loss: 759.7860\n",
      "====> Test set loss: 789.4726\n",
      "====> Epoch: 212 Average loss: 759.7261\n",
      "====> Test set loss: 788.9600\n",
      "====> Epoch: 213 Average loss: 759.6137\n",
      "====> Test set loss: 790.1519\n",
      "====> Epoch: 214 Average loss: 759.8616\n",
      "====> Test set loss: 790.0801\n",
      "====> Epoch: 215 Average loss: 759.8898\n",
      "====> Test set loss: 791.7382\n",
      "====> Epoch: 216 Average loss: 759.1644\n",
      "====> Test set loss: 790.6379\n",
      "====> Epoch: 217 Average loss: 759.1370\n",
      "====> Test set loss: 790.5298\n",
      "====> Epoch: 218 Average loss: 759.7133\n",
      "====> Test set loss: 789.7632\n",
      "====> Epoch: 219 Average loss: 759.4337\n",
      "====> Test set loss: 788.3661\n",
      "====> Epoch: 220 Average loss: 759.3100\n",
      "====> Test set loss: 789.8305\n",
      "====> Epoch: 221 Average loss: 759.5101\n",
      "====> Test set loss: 789.8522\n",
      "====> Epoch: 222 Average loss: 759.7131\n",
      "====> Test set loss: 790.3603\n",
      "====> Epoch: 223 Average loss: 759.7812\n",
      "====> Test set loss: 788.7236\n",
      "====> Epoch: 224 Average loss: 759.4318\n",
      "====> Test set loss: 789.5097\n",
      "====> Epoch: 225 Average loss: 759.4259\n",
      "====> Test set loss: 791.0411\n",
      "====> Epoch: 226 Average loss: 759.4091\n",
      "====> Test set loss: 790.0969\n",
      "====> Epoch: 227 Average loss: 758.5979\n",
      "====> Test set loss: 791.8291\n",
      "====> Epoch: 228 Average loss: 759.3116\n",
      "====> Test set loss: 789.0161\n",
      "====> Epoch: 229 Average loss: 759.1858\n",
      "====> Test set loss: 791.9461\n",
      "====> Epoch: 230 Average loss: 758.6245\n",
      "====> Test set loss: 789.7830\n",
      "====> Epoch: 231 Average loss: 758.9270\n",
      "====> Test set loss: 792.2242\n",
      "====> Epoch: 232 Average loss: 759.0594\n",
      "====> Test set loss: 791.8203\n",
      "====> Epoch: 233 Average loss: 758.9345\n",
      "====> Test set loss: 790.1668\n",
      "====> Epoch: 234 Average loss: 758.5313\n",
      "====> Test set loss: 789.7290\n",
      "====> Epoch: 235 Average loss: 758.3822\n",
      "====> Test set loss: 789.9828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 236 Average loss: 758.3846\n",
      "====> Test set loss: 790.9010\n",
      "====> Epoch: 237 Average loss: 759.2598\n",
      "====> Test set loss: 792.5877\n",
      "====> Epoch: 238 Average loss: 758.3031\n",
      "====> Test set loss: 790.3504\n",
      "====> Epoch: 239 Average loss: 758.6287\n",
      "====> Test set loss: 791.3690\n",
      "====> Epoch: 240 Average loss: 758.2634\n",
      "====> Test set loss: 791.0162\n",
      "====> Epoch: 241 Average loss: 758.7448\n",
      "====> Test set loss: 790.3076\n",
      "====> Epoch: 242 Average loss: 758.3062\n",
      "====> Test set loss: 790.5515\n",
      "====> Epoch: 243 Average loss: 757.6874\n",
      "====> Test set loss: 791.3333\n",
      "====> Epoch: 244 Average loss: 757.7100\n",
      "====> Test set loss: 790.5978\n",
      "====> Epoch: 245 Average loss: 758.5691\n",
      "====> Test set loss: 791.0303\n",
      "====> Epoch: 246 Average loss: 757.6480\n",
      "====> Test set loss: 790.3659\n",
      "====> Epoch: 247 Average loss: 758.4398\n",
      "====> Test set loss: 789.4741\n",
      "====> Epoch: 248 Average loss: 758.0374\n",
      "====> Test set loss: 790.6081\n",
      "====> Epoch: 249 Average loss: 757.8214\n",
      "====> Test set loss: 788.8736\n",
      "====> Epoch: 250 Average loss: 757.5188\n",
      "====> Test set loss: 790.1885\n",
      "====> Epoch: 251 Average loss: 757.9264\n",
      "====> Test set loss: 789.6978\n",
      "====> Epoch: 252 Average loss: 758.0107\n",
      "====> Test set loss: 790.4587\n",
      "====> Epoch: 253 Average loss: 757.8225\n",
      "====> Test set loss: 794.1033\n",
      "====> Epoch: 254 Average loss: 757.6339\n",
      "====> Test set loss: 789.7144\n",
      "====> Epoch: 255 Average loss: 757.9968\n",
      "====> Test set loss: 792.0015\n",
      "====> Epoch: 256 Average loss: 757.7739\n",
      "====> Test set loss: 791.5988\n",
      "====> Epoch: 257 Average loss: 757.6107\n",
      "====> Test set loss: 790.3074\n",
      "====> Epoch: 258 Average loss: 757.5400\n",
      "====> Test set loss: 789.1894\n",
      "====> Epoch: 259 Average loss: 757.7543\n",
      "====> Test set loss: 790.9469\n",
      "====> Epoch: 260 Average loss: 757.4586\n",
      "====> Test set loss: 790.5589\n",
      "====> Epoch: 261 Average loss: 757.6692\n",
      "====> Test set loss: 791.5935\n",
      "====> Epoch: 262 Average loss: 757.3198\n",
      "====> Test set loss: 791.2551\n",
      "====> Epoch: 263 Average loss: 757.0593\n",
      "====> Test set loss: 790.3041\n",
      "====> Epoch: 264 Average loss: 757.4790\n",
      "====> Test set loss: 791.3378\n",
      "====> Epoch: 265 Average loss: 756.9795\n",
      "====> Test set loss: 791.9246\n",
      "====> Epoch: 266 Average loss: 756.8745\n",
      "====> Test set loss: 791.4807\n",
      "====> Epoch: 267 Average loss: 757.2108\n",
      "====> Test set loss: 791.4718\n",
      "====> Epoch: 268 Average loss: 757.1611\n",
      "====> Test set loss: 790.1906\n",
      "====> Epoch: 269 Average loss: 757.0264\n",
      "====> Test set loss: 790.4694\n",
      "====> Epoch: 270 Average loss: 757.2288\n",
      "====> Test set loss: 791.6300\n",
      "====> Epoch: 271 Average loss: 757.4905\n",
      "====> Test set loss: 791.9965\n",
      "====> Epoch: 272 Average loss: 756.6756\n",
      "====> Test set loss: 791.1023\n",
      "====> Epoch: 273 Average loss: 756.3568\n",
      "====> Test set loss: 791.7811\n",
      "====> Epoch: 274 Average loss: 756.6464\n",
      "====> Test set loss: 791.0793\n",
      "====> Epoch: 275 Average loss: 756.4765\n",
      "====> Test set loss: 790.3862\n",
      "====> Epoch: 276 Average loss: 756.6133\n",
      "====> Test set loss: 791.4003\n",
      "====> Epoch: 277 Average loss: 756.4144\n",
      "====> Test set loss: 792.2419\n",
      "====> Epoch: 278 Average loss: 756.8514\n",
      "====> Test set loss: 790.6520\n",
      "====> Epoch: 279 Average loss: 757.1140\n",
      "====> Test set loss: 791.7731\n",
      "====> Epoch: 280 Average loss: 756.4161\n",
      "====> Test set loss: 790.6543\n",
      "====> Epoch: 281 Average loss: 756.7822\n",
      "====> Test set loss: 791.3007\n",
      "====> Epoch: 282 Average loss: 756.7504\n",
      "====> Test set loss: 791.6050\n",
      "====> Epoch: 283 Average loss: 755.9463\n",
      "====> Test set loss: 790.4534\n",
      "====> Epoch: 284 Average loss: 756.4075\n",
      "====> Test set loss: 793.2731\n",
      "====> Epoch: 285 Average loss: 756.4539\n",
      "====> Test set loss: 791.2811\n",
      "====> Epoch: 286 Average loss: 756.6688\n",
      "====> Test set loss: 790.7141\n",
      "====> Epoch: 287 Average loss: 756.4990\n",
      "====> Test set loss: 792.0801\n",
      "====> Epoch: 288 Average loss: 756.2093\n",
      "====> Test set loss: 791.2291\n",
      "====> Epoch: 289 Average loss: 755.6059\n",
      "====> Test set loss: 791.0741\n",
      "====> Epoch: 290 Average loss: 756.0298\n",
      "====> Test set loss: 790.8944\n",
      "====> Epoch: 291 Average loss: 755.8506\n",
      "====> Test set loss: 791.0790\n",
      "====> Epoch: 292 Average loss: 756.2893\n",
      "====> Test set loss: 790.9088\n",
      "====> Epoch: 293 Average loss: 756.6288\n",
      "====> Test set loss: 792.5263\n",
      "====> Epoch: 294 Average loss: 756.0381\n",
      "====> Test set loss: 792.0846\n",
      "====> Epoch: 295 Average loss: 756.3690\n",
      "====> Test set loss: 792.1079\n",
      "====> Epoch: 296 Average loss: 755.8916\n",
      "====> Test set loss: 790.1040\n",
      "====> Epoch: 297 Average loss: 756.3446\n",
      "====> Test set loss: 790.2105\n",
      "====> Epoch: 298 Average loss: 756.1595\n",
      "====> Test set loss: 791.2968\n",
      "====> Epoch: 299 Average loss: 756.2680\n",
      "====> Test set loss: 791.6897\n",
      "====> Epoch: 300 Average loss: 755.5838\n",
      "====> Test set loss: 792.0981\n"
     ]
    }
   ],
   "source": [
    "import warnings;\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    with torch.no_grad():\n",
    "        if(epoch > epochs - 10):\n",
    "            sample = torch.randn(64, l_d).to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.view(64, 1, 28, 28),\n",
    "                       'results/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGudJREFUeJzt3X2QFfWd7/HPF+aBYWCYQUZAQGAM\nwURFxBFFXZOYbKLEyL27uGWS3WyyqWJ1s9ncSt1K3NrarU1qa6uSW9kkxnu12CRbm6sxezUxeveq\nq8n6uBswg4KCgDxIhBFxEAEhPMzD9/7x7SPD4ZwzZ4aZ6dOH96tqqs/06Tnz7Wn49K9//etuc3cB\nAKrLmLQLAAAMP8IdAKoQ4Q4AVYhwB4AqRLgDQBUi3AGgCqUa7mb2QzN708zWl7Hst81sbfL1ipnt\nH40aASCLLM1x7mZ2jaRDkn7k7hcO4ue+KOkSd/+TESsOADIs1Za7uz8taV//eWZ2npk9amZrzOwZ\nMzu/wI9+UtK9o1IkAGRQTdoFFLBS0i3uvsXMLpf0vyRdm3vTzGZLmivp31OqDwAqXkWFu5lNkHSl\npPvMLDe7Pm+xmyXd7+69o1kbAGRJRYW7optov7svLLHMzZK+MEr1AEAmVdRQSHc/KOlVM7tJkixc\nnHvfzOZLapH0q5RKBIBMSHso5L2KoJ5vZrvM7POSPi3p82a2TtIGScv6/cgnJf3EuZUlAJSU6lBI\nAMDIqKhuGQDA8EjthOqUKVN8zpw5af16AMikNWvW7HX31oGWSy3c58yZo46OjrR+PQBkkpn9ppzl\n6JYBgCpEuANAFSLcAaAKEe4AUIUIdwCoQoQ7AFQhwh0AqlD2wn39eumv/1rq6kq7EgCoWNkL902b\npL/7O2nPnrQrAYCKlb1wr62N6fHj6dYBABUse+FeVxdTwh0AispuuHd3p1sHAFSw7IU73TIAMKDs\nhTstdwAYUPbCnZY7AAwoe+HOCVUAGFB2w51uGQAoqqxwN7NmM7vfzDaZ2UYzW5L3/gfN7ICZrU2+\n/mZkyhXdMgBQhnIfs/ddSY+6+3Izq5M0vsAyz7j7DcNXWhG03AFgQAOGu5k1SbpG0mclyd2PS0qv\n2UzLHQAGVE63TJukLkn/ZGYvmNn3zayxwHJLzGydmT1iZhcU+iAzW2FmHWbW0TXUG39xQhUABlRO\nuNdIWiTpTne/RNJhSbflLfO8pNnufrGk70n6eaEPcveV7t7u7u2tra1Dq5huGQAYUDnhvkvSLndf\nnXx/vyLs3+XuB939UPL6YUm1ZjZlWCvNoVsGAAY0YLi7+xuSdprZ/GTWhyW93H8ZM5tmZpa8Xpx8\n7lvDXGvIhTstdwAoqtzRMl+UdE8yUma7pM+Z2S2S5O53SVou6VYz65F0RNLN7u4jUbDMpJoaWu4A\nUEJZ4e7uayW1582+q9/7d0i6YxjrKq2ujnAHgBKyd4WqFOFOtwwAFJXNcK+tpeUOACVkM9xpuQNA\nSdkMd1ruAFBSNsOdE6oAUFJ2w51uGQAoKpvhTrcMAJSUzXCn5Q4AJWUz3Gm5A0BJ2Qx3TqgCQEnZ\nDXe6ZQCgqGyGO90yAFBSNsOdljsAlJTNcKflDgAlZTPcOaEKACVlM9xra+mWAYASshnutNwBoKTs\nhjstdwAoKpvhzglVACgpm+FOtwwAlJTNcK+tlXp7pb6+tCsBgIqUzXCvq4sp/e4AUBDhDgBVKJvh\nXlsbU/rdAaCgbIZ7ruVOuANAQdkM91zLnW4ZACgom+FOyx0ASsp2uNNyB4CCshnuuW6ZY8fSrQMA\nKlQ2w72hIaZHjqRbBwBUqGyG+4QJMT18ON06AKBCZTPcGxtjSrgDQEHZDvdDh9KtAwAqVDbDnW4Z\nACgpm+FOtwwAlES4A0AVyma419XFWHf63AGgoGyGuxStd1ruAFBQWeFuZs1mdr+ZbTKzjWa2JO99\nM7PbzWyrmb1oZotGptx+CHcAKKqmzOW+K+lRd19uZnWSxue9f72kecnX5ZLuTKYjZ8IEumUAoIgB\nW+5m1iTpGkk/kCR3P+7u+/MWWybpRx5WSWo2s+nDXm1/tNwBoKhyumXaJHVJ+icze8HMvm9mjXnL\nzJC0s9/3u5J5JzGzFWbWYWYdXV1dQy5aEuEOACWUE+41khZJutPdL5F0WNJtectYgZ/zU2a4r3T3\ndndvb21tHXSxJ2lspFsGAIooJ9x3Sdrl7quT7+9XhH3+MrP6fT9T0uunX14JEybQcgeAIgYMd3d/\nQ9JOM5ufzPqwpJfzFntI0meSUTNXSDrg7ruHt9Q8dMsAQFHljpb5oqR7kpEy2yV9zsxukSR3v0vS\nw5KWStoq6beSPjcCtZ6MbhkAKKqscHf3tZLa82bf1e99l/SFYaxrYHTLAEBR2b5C9dgxqacn7UoA\noOJkO9wlWu8AUEB2w517ugNAUdkNd1ruAFBU9sOdETMAcIrshvv45N5lR46kWwcAVKDshvu4cTE9\nejTdOgCgAmU33BsaYkq4A8ApshvuuZY73TIAcIrshzstdwA4RXbDPdctQ8sdAE6R3XCn5Q4ARRHu\nAFCFshvudMsAQFHZDfeaGmnsWFruAFBAdsNdiq4Zwh0ATpHtcG9ooFsGAArIdrjTcgeAgrId7g0N\nhDsAFJDtcB83jm4ZACgg++FOyx0ATpHtcKdbBgAKyna40y0DAAVlP9xpuQPAKbId7oxzB4CCsh3u\ntNwBoCDCHQCqULbDnW4ZACgo2+FOyx0ACsp+uHd3S729aVcCABUl2+Gee2AHrXcAOEm2w51H7QFA\nQYQ7AFShbIc7z1EFgIKyHe603AGgoGyHOy13ACgo2+He2BjTQ4fSrQMAKky2w72lJab796dbBwBU\nmJpyFjKzHZLekdQrqcfd2/Pe/6CkByW9msz6mbt/ffjKLKK5OaaEOwCcpKxwT3zI3feWeP8Zd7/h\ndAsalFy4v/32qP5aAKh02e6WaWqSzGi5A0CecsPdJT1mZmvMbEWRZZaY2Toze8TMLii0gJmtMLMO\nM+vo6uoaUsEnGTNGmjSJcAeAPOV2y1zl7q+b2dmSHjezTe7+dL/3n5c0290PmdlSST+XNC//Q9x9\npaSVktTe3u6nWXtobibcASBPWS13d389mb4p6QFJi/PeP+juh5LXD0uqNbMpw1xrYc3N9LkDQJ4B\nw93MGs1sYu61pI9KWp+3zDQzs+T14uRz3xr+cgtoaaHlDgB5yumWmSrpgSS7ayT92N0fNbNbJMnd\n75K0XNKtZtYj6Yikm919eLpdBtLcLG3ZMiq/CgCyYsBwd/ftki4uMP+ufq/vkHTH8JZWJvrcAeAU\n2R4KKdHnDgAFZD/cW1qkw4fjcXsAAEnVEO65q1QPHEi3DgCoINUT7vS7A8C7qifc6XcHgHdlP9yn\nJNdKDcftDACgSmQ/3KdPj+kbb6RbBwBUkOyH+7RpMd29O906AKCCZD/cx42LfnfCHQDelf1wl6Jr\nhnAHgHcR7gBQhaoj3KdN44QqAPRTHeGea7mP0o0oAaDSVU+4Hz3KLQgAIFE94S7R7w4AieoK987O\ndOsAgApRHeF+3nkx3bYt3ToAoEJUR7jPnBkXM/G4PQCQVC3hPmZMtN4JdwCQVC3hLknz5hHuAJCo\nrnDftk3q7U27EgBIXXWF+/Hj0s6daVcCAKmrnnB/73tjunlzunUAQAWonnC/6KKYvvhiunUAQAWo\nnnCfPDmGRK5dm3YlAJC66gl3SVq4UFq3Lu0qACB11RXuF18sbdoUNxEDgDNY9YV7b6+0YUPalQBA\nqqor3BctimlHR7p1AEDKqivc29qkKVOk1avTrgQAUlVd4W4mLV5MuAM441VXuEvS5ZdLGzdKBw+m\nXQkApKY6w91d+vWv064EAFJTfeF+xRVxC+Cnnkq7EgBITfWF+6RJ0qWXSk88kXYlAJCa6gt3SfrQ\nh+Kk6uHDaVcCAKmo3nDv7paeeSbtSgAgFdUZ7h/4gNTYKD3wQNqVAEAqqjPcGxqkT3xC+tnPpJ6e\ntKsBgFFXVrib2Q4ze8nM1prZKdf2W7jdzLaa2Ytmtmj4Sx2km26S9u6Vnnwy7UoAYNQNpuX+IXdf\n6O7tBd67XtK85GuFpDuHo7jTcv310TVz331pVwIAo264umWWSfqRh1WSms1s+jB99tDQNQPgDFZu\nuLukx8xsjZmtKPD+DEn9n0y9K5l3EjNbYWYdZtbR1dU1+GoHK9c188tfjvzvAoAKUm64X+XuixTd\nL18ws2vy3rcCP+OnzHBf6e7t7t7e2to6yFKHYOlSado06RvfGPnfBQAVpKxwd/fXk+mbkh6QtDhv\nkV2SZvX7fqak14ejwNMybpz01a/G1aqMeQdwBhkw3M2s0cwm5l5L+qik9XmLPSTpM8momSskHXD3\n3cNe7VD86Z9KU6dKX/ta2pUAwKgpp+U+VdKzZrZO0nOS/p+7P2pmt5jZLckyD0vaLmmrpH+U9Gcj\nUu1QNDRIX/lK9LvTegdwhjD3U7rGR0V7e7t3jNbj8A4flubPjxb8c89JY8eOzu8FgGFmZmuKDEk/\nSXVeoZqvsVH61rek55+Xbr897WoAYMSdGeEuSX/wB9KNN0q33cYDtAFUvTMn3M2kH/xAmj49rl59\n+eW0KwKAEXPmhLskTZki/eIXUk2N9JGPSNu2pV0RAIyIMyvcJek974mAP3YsLnLaty/tigBg2J15\n4S5JF1wgPfigtGOH9Lu/G7coAIAqcmaGuyRdfXUE/MsvSx/8oLS7Mq65AoDhcOaGuyRdd5308MPR\ngr/mmpgCQBU4s8NdiuetPvaY1NUlLVoUrwEg4wh3Sbryyhj7fu650rJl0r33Sr29aVcFAENGuOfk\nRtHMny996lPSkiWMhQeQWYR7f1OmSGvWSHffLW3dKl14oXTLLdI776RdGQAMCuGeb+xY6dOfljZv\nlr70JWnlSumii6Sf/pTH9QHIDMK9mNZW6dvflp59Vqqvl5YvlxYsiNE1fX1pVwcAJRHuA7nySmnD\nBum++6Tf/lb6+MejJf/II1JKt0sGgIEQ7uWoqYmW++bN0o9/LB0/Hrcu+NjHortm+/a0KwSAkxDu\ng1FfL33yk9GS/853Yvjk8uXS+94n/dEfSd/8pvTWW2lXCQCE+5DU1cXJ1h074slON90Uj/H76ldj\nxE17u/Sv/yq98UbalQI4QxHup6OpSbrsshg6+frr0gsvSH//9xHqn/hE3Dt+yRLpu99lOCWAUXVm\nPEN1tB06FC36VauiT/7556Vx46TLL5cuvjimv/d7MQ8ABqHcZ6gS7qNh9eq4pcGqVdJLL8Wom4kT\n4142ixZJixdLN9wgTZiQdqUAKhzhXqn6+qQnnzzRol+3TjpyJE7WLl4cX01N0rx5cVviWbPSrhhA\nBSk33GtGoxj0M2aMdO218SXFVa+rVkkPPBAXTH3vezHUMqetTfrAB+Ke80uWSOPHSzNmpFI6gOyg\n5V6Jjh2TNm6Unn46WvlPPXXy4wBvuCGeIFVXFzuJhgbpnHPi1gkAqhrdMtWkr09avz7G1b/2Woyx\nP3Dg5GUmT46unBkz4uucc6SWlrhlwrnnxsidMQyOArKOcK9m7vHc1wMHpH/7N8kshmHu2CF1dsaw\nzPzwnz49hmdOnhzj8GfNip3G+98fffwAMoE+92pmFjc2a22N+9AXcvhwXC27Zk08H/aJJ6R77omT\nt/1vfFZbGwE/Y0bsFFpa4pGDF10kXXppPLSkp0eaNk2aNGl01g/AaaPlfibp7Y1W/+OPS93d0U3z\n9NPxUJLOTumss6Rdu+IeOvnq6yPwJ02K+9xL8Vnu0uzZ0q23xsleACOKbhkMXXe39Mwz0c1TWxsn\nan/1K2nLlrj6dvv2OHrIfe3fHzuK5uZ4ktWhQ/H9kiXR5y/FqJ+zzor78IwfHz8HYNDolsHQ1dae\nGKqZ86lPFV/+2WelRx89EfxTp8aFWnffHUGfb/x46fzz4722Num9741+/+nTozvpwgvj/Vmz4o6c\nAAaN/zk4fVdfHV/5enqiv99M+s1vpD17ostn9+6Yzp0rbdsm/cd/RKjnPwSlpkaaOTO6fiZPjp1H\nc3N0A82eHaOAzGJn8p73xHDROXOiC2nSJOnVV2NHddZZ8Tn19aPy5wAqAeGOkVNTc+IK25kzSy/b\n0xPh39AQt2jYujWC/7XXoounqyvuy3PwYOwoOjpixFC5WltjtNDevbFDyF0X0NISO4RzzomdzdSp\nsaMZPz6OLLq7Y8fS3DzkPwOQBvrckV1HjsR09+7YGUyaJL3ySoT3229HYI8ZE68ffDDu8TNp0omd\nRW9v7CwGemyiWRwlTJoURwyHDklnnx33Atq1S1qxImpxj53ZsWNxvmLBAumSS+Jis/3748rjV16J\nHcWFF3LeAUPCCVWgHN3d0s6d0eWzYUPsCCZOlI4ejWldXQT1li3R6u/sjPMDr70W3zc1xRFHMTU1\n8Tlvv33y/Llz44hg3LjYSTQ0RLdRT08cZUyZEiex58yJ8w+TJ5/4am6OYa6trfG5TU0xnLW2diT/\nUqgQhDsw0vr6ogunoyOOEqTYUdTWRjfUunVxncHevXH1cG1tjCbq7Iwjid7e2IkcOhTTo0djZ7Bn\nT4T3+efHtQcHDw5ci1nsKGbNkhYujKOHrVvj85qa4rMXLJCuuy52ZAcPxk5n4cL4Xe+8c+LEdk9P\nLNPSEtc6dHfHDm7atPiZd96JGg8fjnMd48bF0RBXQI8Kwh3IsmPHoiXvHuG/b9/JXy0tEfzNzRHU\nW7bECKVNm+JkdV2ddN550RV05EgE8JNPxucORm4EU24HU1MT4Z9v3Li4wd2aNbEDuPLKqHv37jgy\nWbAgurKOHpV+/nPpqqui9rPPjhPfbW3xXOKWlrhjaltb7BT7+uL6ivnzo2urs/PEzfP27Yt1mzkz\nfkd394kapart9iLcAZzsjTfiHMHs2dFC3707dgpNTRGOO3acGLp6zjnx3lNPRZhedlmE7dtvR5fR\n2WfHzmfbtjgC2bEjLo77nd+JAP/P/4wupBkz4lYY69bFfCmOFl56KXZcfX2xY8i9V8yYMaXPjdTX\nR7ibxWuzuKZizJjY6V12WezYdu6Mz3GPI5C+vnj+8dy5sX6NjbFc7sR+Z2eMBMud7F+6NP6GR47E\nTs49dkSLFsXvqa2NiwLXrZMuuCB2rueeG3/7sWNj+fPPP61nNxDuACqHewTd8ePRsj94MHYae/fG\nzuK11+I+SYcOxQ5i69Y4apg2LUJ2w4Zo1be1RYh2dsbnTJwYYbtvX+wkenpO3GJj/fr4vq4ufr6m\nJkJ87NgI//Hj48T6qlWFa66vjx1fV9fA6zdhQuFrOgppbJS+/nXpy18u/+/Xz7BfxGRmYyV1SOp0\n9xvy3vuspP8hqTOZdYe7f7/8cgFUtVyLOnetQe5mddOmxbStLW5hkXPFFSdeX3DByNa2e3e0rHMX\n39XXR1fUeedFy3/79jja2bNHeuSRaHk3N8fOIrcTeeKJaKH39cW5i4suitZ7U1Mc3cyYETuV3t74\njFF4CE/ZLXcz+7KkdklNRcK93d3/vNxfTMsdAAav3JZ7Wae3zWympI9LojUOABlQ7til70j6iqRS\nV3v8vpm9aGb3m1nBYw4zW2FmHWbW0VVOPxYAYEgGDHczu0HSm+6+psRi/1fSHHdfIOkXkv650ELu\nvtLd2929vbW1dUgFAwAGVk7L/SpJN5rZDkk/kXStmd3dfwF3f8vdcwNo/1HSpcNaJQBgUAYMd3f/\nS3ef6e5zJN0s6d/d/Q/7L2Nm0/t9e6OkjcNaJQBgUIZ8V0gz+7qkDnd/SNJfmNmNknok7ZP02eEp\nDwAwFFzEBAAZMqxDIQEA2ZJay93MuiT9Zog/PkXSIJ7UUNFYl8rEulQm1kWa7e4DDjdMLdxPh5l1\nlHNYkgWsS2ViXSoT61I+umUAoAoR7gBQhbIa7ivTLmAYsS6ViXWpTKxLmTLZ5w4AKC2rLXcAQAmE\nOwBUocyFu5ldZ2abzWyrmd2Wdj2DZWY7zOwlM1trZh3JvMlm9riZbUmmLWnXWYiZ/dDM3jSz9f3m\nFazdwu3JdnrRzBalV/mpiqzL35pZZ7Jt1prZ0n7v/WWyLpvN7GPpVH0qM5tlZk+Y2UYz22BmX0rm\nZ267lFiXLG6XcWb2nJmtS9bla8n8uWa2Otku/2Jmdcn8+uT7rcn7c067CHfPzJeksZK2SWqTVCdp\nnaT3p13XINdhh6QpefO+Kem25PVtkr6Rdp1Far9G0iJJ6weqXdJSSY9IMklXSFqddv1lrMvfSvrv\nBZZ9f/JvrV7S3OTf4Ni01yGpbbqkRcnriZJeSerN3HYpsS5Z3C4maULyulbS6uTv/X8k3ZzMv0vS\nrcnrP5N0V/L6Zkn/cro1ZK3lvljSVnff7u7HFbcgXpZyTcNhmU7cA/+fJf2XFGspyt2fVtwYrr9i\ntS+T9CMPqyQ15909NFVF1qWYZZJ+4u7H3P1VSVsV/xZT5+673f355PU7ijuyzlAGt0uJdSmmkreL\nu3vuidm1yZdLulbS/cn8/O2S2173S/qwmdnp1JC1cJ8haWe/73ep9MavRC7pMTNbY2YrknlT3X23\nFP/AJZ2dWnWDV6z2rG6rP0+6K37Yr3ssE+uSHMpfomglZnq75K2LlMHtYmZjzWytpDclPa44stjv\n7j3JIv3rfXddkvcPSDrrdH5/1sK90J4sa2M5r3L3RZKul/QFM7sm7YJGSBa31Z2SzpO0UNJuSd9K\n5lf8upjZBEk/lfTf3P1gqUULzKv0dcnkdnH3XndfKGmm4ojifYUWS6bDvi5ZC/ddkvo/n3WmpNdT\nqmVI3P31ZPqmpAcUG31P7tA4mb6ZXoWDVqz2zG0rd9+T/IfsUzxRLHeIX9HrYma1ijC8x91/lszO\n5HYptC5Z3S457r5f0pOKPvdmM8s9R6N/ve+uS/L+JJXfbVhQ1sL915LmJWec6xQnHh5KuaaymVmj\nmU3MvZb0UUnrFevwx8lifyzpwXQqHJJitT8k6TPJ6IwrJB3IdRNUqry+5/+q2DZSrMvNyYiGuZLm\nSXputOsrJOmX/YGkje7+D/3eytx2KbYuGd0urWbWnLxukPQRxTmEJyQtTxbL3y657bVc8cS70zsK\nSfus8hDOQi9VnEXfJumv0q5nkLW3Kc7ur5O0IVe/om/tl5K2JNPJaddapP57FYfF3YqWxueL1a44\nzPyfyXZ6SVJ72vWXsS7/O6n1xeQ/2/R+y/9Vsi6bJV2fdv396rpacfj+oqS1ydfSLG6XEuuSxe2y\nQNILSc3rJf1NMr9NsQPaKuk+SfXJ/HHJ91uT99tOtwZuPwAAVShr3TIAgDIQ7gBQhQh3AKhChDsA\nVCHCHQCqEOEOAFWIcAeAKvT/Ac+y/CkUyMB3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(train_losses),'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(test_losses),'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
